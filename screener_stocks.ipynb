{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMizVAw9Zk0keHq11+TYGn1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnanthSrinivasan/TickerFetcherDemo/blob/main/screener_stocks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Imports & Setup\n",
        "# ----------------------------\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "import random\n",
        "\n",
        "FINVIZ_BASE = \"https://finviz.com\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    )\n",
        "}\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update(HEADERS)\n",
        "\n",
        "# ----------------------------\n",
        "# Part 1: Screener Fetch & Save\n",
        "# ----------------------------\n",
        "screener_urls = {\n",
        "    \"10% Change\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=151\"\n",
        "        f\"&f=ind_stocksonly,sh_avgvol_o500,sh_price_o5,ta_changeopen_u10,\"\n",
        "        f\"ta_sma20_sa50,ta_sma50_pa&ft=4&o=-relativevolume&\"\n",
        "        f\"c=0,1,2,3,4,5,6,64,67,65,66\"\n",
        "    ),\n",
        "    \"Growth\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=an_recom_buybetter,fa_epsqoq_o20,fa_salesqoq_o20,\"\n",
        "        f\"ind_stocksonly,sh_avgvol_o1000,sh_price_o10,ta_perf_4wup,\"\n",
        "        f\"ta_perf2_13wup,ta_sma20_pa,ta_sma200_pa,ta_sma50_pa&ft=4\"\n",
        "    ),\n",
        "    \"IPO\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=cap_midover,ind_stocksonly,ipodate_prev3yrs,sh_avgvol_o1000,\"\n",
        "        f\"sh_price_o10,ta_beta_o0.5,ta_sma20_pa&ft=4\"\n",
        "    ),\n",
        "    \"52 Week High\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=ind_stocksonly,sh_avgvol_o1000,sh_price_o10,ta_beta_o1,\"\n",
        "        f\"ta_highlow52w_nh&ft=4\"\n",
        "    ),\n",
        "    \"Week 20%+ Gain\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=cap_smallover,ind_stocksonly,sh_avgvol_o1000,ta_perf_1w30o,\"\n",
        "        f\"ta_sma20_pa,ta_volatility_wo4&ft=4&o=-marketcap\"\n",
        "    )\n",
        "}\n",
        "\n",
        "def fetch_all_tickers(screener_url: str, max_pages: int = 10) -> pd.DataFrame:\n",
        "    combined = []\n",
        "    seen = set()\n",
        "    page = 1\n",
        "\n",
        "    while page <= max_pages:\n",
        "        resp = session.get(f\"{screener_url}&r={1+(page-1)*20}\", timeout=10)\n",
        "        if resp.status_code != 200:\n",
        "            break\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "        rows = soup.select('tr[valign=\"top\"]')\n",
        "        if not rows:\n",
        "            break\n",
        "\n",
        "        new_data = False\n",
        "        for row in rows:\n",
        "            cols = row.find_all('td')\n",
        "            if len(cols) == 11:\n",
        "                ticker = cols[1].text.strip()\n",
        "                if ticker and ticker not in seen:\n",
        "                    combined.append([c.text.strip() for c in cols])\n",
        "                    seen.add(ticker)\n",
        "                    new_data = True\n",
        "        if not new_data:\n",
        "            break\n",
        "        page += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "    columns = ['No.', 'Ticker', 'Company', 'Sector', 'Industry',\n",
        "               'Country', 'Market Cap', 'P/E', 'Volume', 'Price', 'Change']\n",
        "    return pd.DataFrame(combined, columns=columns) if combined else pd.DataFrame(columns=columns)\n",
        "\n",
        "def aggregate_and_save(screener_map: dict) -> (pd.DataFrame, str, str):\n",
        "    mapping = defaultdict(list)\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    for name, url in screener_map.items():\n",
        "        df = fetch_all_tickers(url)\n",
        "        print(f\"{name}: {len(df)} tickers found\")\n",
        "        for t in df['Ticker'].unique():\n",
        "            mapping[t].append(name)\n",
        "\n",
        "    if not mapping:\n",
        "        mapping['TSLA'].append('Default Screener')\n",
        "\n",
        "    data = []\n",
        "    for t, screens in mapping.items():\n",
        "        data.append({\n",
        "            'Ticker': t,\n",
        "            'Appearances': len(screens),\n",
        "            'Screeners': \", \".join(screens)\n",
        "        })\n",
        "    summary_df = pd.DataFrame(data).sort_values(['Appearances','Ticker'], ascending=[False, True])\n",
        "\n",
        "    csv_file = f\"finviz_screeners_{today}.csv\"\n",
        "    html_file = f\"finviz_screeners_{today}.html\"\n",
        "    summary_df.to_csv(csv_file, index=False)\n",
        "    summary_df.to_html(html_file, index=False)\n",
        "\n",
        "    return summary_df, csv_file, html_file\n",
        "\n",
        "# ----------------------------\n",
        "# Part 2: Snapshot Fetch with Retries\n",
        "# ----------------------------\n",
        "def get_snapshot_metrics(ticker: str, max_retries: int = 5):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            resp = session.get(f\"{FINVIZ_BASE}/quote.ashx\", params={\"t\": ticker})\n",
        "            resp.raise_for_status()\n",
        "            soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "            table = soup.find(\"table\", class_=\"snapshot-table2\")\n",
        "            if not table:\n",
        "                raise ValueError(\"Snapshot table not found\")\n",
        "\n",
        "            data = {}\n",
        "            for row in table.find_all(\"tr\"):\n",
        "                cells = row.find_all(\"td\")\n",
        "                for key_cell, val_cell in zip(cells[0::2], cells[1::2]):\n",
        "                    key = key_cell.get_text(strip=True).rstrip('.')\n",
        "                    data[key] = val_cell.get_text(strip=True)\n",
        "\n",
        "            atr_pct = float(data.get(\"ATR (14)\", 0)) / float(data.get(\"Price\", \"1\").replace(',', '')) * 100\n",
        "            eps_str = data.get(\"EPS Y/Y TTM\", '0').replace('%','').strip()\n",
        "            eps = float(eps_str) if eps_str != '-' else 0.0\n",
        "\n",
        "            sales_str = data.get(\"Sales Y/Y TTM\", '0').replace('%','').strip()\n",
        "            sales = float(sales_str) if sales_str != '-' else 0.0\n",
        "            return atr_pct, eps, sales\n",
        "        except requests.HTTPError as e:\n",
        "            if e.response.status_code == 429:\n",
        "                wait = (2 ** attempt) + random.random()\n",
        "                print(f\"Rate limited fetching snapshot for {ticker}; retrying in {wait:.1f}s…\")\n",
        "                time.sleep(wait)\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"HTTP error for {ticker}: {e}\")\n",
        "                break\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching snapshot for {ticker}: {e}\")\n",
        "            break\n",
        "\n",
        "    return None, None, None\n",
        "\n",
        "# ----------------------------\n",
        "# Part 3: Chart Gallery\n",
        "# ----------------------------\n",
        "def generate_finviz_gallery(tickers: list) -> str:\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    out_html = f\"finviz_chart_grid_{today}.html\"\n",
        "    html = [\n",
        "        '<html><head><title>Finviz Chart Gallery</title>',\n",
        "        '<style>',\n",
        "        'body { font-family: Arial; background: #f5f5f5; padding: 20px }',\n",
        "        '.chart-grid { display: flex; flex-wrap: wrap; gap: 20px }',\n",
        "        '.chart-item { width: 23%; background: white; border:1px solid #ccc;',\n",
        "        'padding:10px; box-shadow:2px 2px 5px rgba(0,0,0,0.1); text-align:center }',\n",
        "        '.chart-item img { max-width:100%; height:auto }',\n",
        "        'h2 { text-align:center }',\n",
        "        '</style></head><body><h2>Finviz Chart Gallery</h2>',\n",
        "        '<div class=\"chart-grid\">'\n",
        "    ]\n",
        "    for t in tickers:\n",
        "        url = f\"{FINVIZ_BASE}/chart.ashx\"\n",
        "        params = {\"t\": t, \"ty\": \"c\", \"ta\": 1, \"p\": \"d\", \"s\": \"m\"}\n",
        "        req = requests.Request('GET', url, params=params).prepare()\n",
        "        html.append(f'<div class=\"chart-item\"><h4>{t}</h4>'\n",
        "                    f'<img src=\"{req.url}\" alt=\"{t}\"></div>')\n",
        "    html.append('</div></body></html>')\n",
        "\n",
        "    with open(out_html, 'w') as f:\n",
        "        f.write(\"\\n\".join(html))\n",
        "    return out_html\n",
        "\n",
        "# ----------------------------\n",
        "# Part 4: Main Execution with Logging\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    summary_df, csv_path, html_summary = aggregate_and_save(screener_urls)\n",
        "    print(f\"Summary CSV: {csv_path}\\nSummary HTML: {html_summary}\")\n",
        "\n",
        "    print(f\"Initial tickers count: {len(summary_df)}\")\n",
        "\n",
        "    # Fetch snapshot metrics\n",
        "    summary_df[['ATR%', 'EPS Y/Y TTM', 'Sales Y/Y TTM']] = summary_df['Ticker'].apply(\n",
        "        lambda t: pd.Series(get_snapshot_metrics(t))\n",
        "    )\n",
        "\n",
        "    filter_a = summary_df[summary_df['ATR%'] > 3.0]\n",
        "    print(f\"Tickers with ATR% > 3.0: {len(filter_a)}\")\n",
        "\n",
        "    # Ask user whether to apply EPS and Sales filters\n",
        "    apply_earnings_filter = input(\"Filter further by EPS > 20 and Sales > 20? (y/n): \").strip().lower()\n",
        "    if apply_earnings_filter == 'y':\n",
        "        final_filtered = filter_a[\n",
        "            (filter_a['EPS Y/Y TTM'] > 20) &\n",
        "            (filter_a['Sales Y/Y TTM'] > 20)\n",
        "        ]\n",
        "        print(f\"Tickers with ATR% > 3.0 AND EPS & Sales Y/Y TTM > 20: {len(final_filtered)}\")\n",
        "    else:\n",
        "        final_filtered = filter_a\n",
        "        print(\"Skipped EPS/Sales filtering. Using ATR% > 3.0 only.\")\n",
        "\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    out_file = f\"finviz_filtered_{today}.csv\"\n",
        "    final_filtered.to_csv(out_file, index=False)\n",
        "    print(f\"Filtered results saved: {out_file}\")\n",
        "\n",
        "    gallery_path = generate_finviz_gallery(final_filtered['Ticker'].tolist())\n",
        "    print(f\"Filtered chart gallery HTML: {gallery_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni9NW-JZ1AMC",
        "outputId": "be87f9e1-29f5-4cfe-87ee-30c704491ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10% Change: 9 tickers found\n",
            "Growth: 44 tickers found\n",
            "IPO: 10 tickers found\n",
            "52 Week High: 7 tickers found\n",
            "Week 20%+ Gain: 0 tickers found\n",
            "Summary CSV: finviz_screeners_2025-08-03.csv\n",
            "Summary HTML: finviz_screeners_2025-08-03.html\n",
            "Initial tickers count: 63\n",
            "Rate limited fetching snapshot for BSX; retrying in 1.4s…\n",
            "Rate limited fetching snapshot for NI; retrying in 1.3s…\n",
            "Rate limited fetching snapshot for WK; retrying in 1.0s…\n",
            "Tickers with ATR% > 3.0: 42\n",
            "Filter further by EPS > 20 and Sales > 20? (y/n): n\n",
            "Skipped EPS/Sales filtering. Using ATR% > 3.0 only.\n",
            "Filtered results saved: finviz_filtered_2025-08-03.csv\n",
            "Filtered chart gallery HTML: finviz_chart_grid_2025-08-03.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Imports & Setup\n",
        "# ----------------------------\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "import random\n",
        "\n",
        "FINVIZ_BASE = \"https://finviz.com\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    )\n",
        "}\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update(HEADERS)\n",
        "\n",
        "# ----------------------------\n",
        "# Part 1: Screener Fetch & Save\n",
        "# ----------------------------\n",
        "screener_urls = {\n",
        "    \"10% Change\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=151\"\n",
        "        f\"&f=ind_stocksonly,sh_avgvol_o500,sh_price_o5,ta_changeopen_u10,\"\n",
        "        f\"ta_sma20_sa50,ta_sma50_pa&ft=4&o=-relativevolume&\"\n",
        "        f\"c=0,1,2,3,4,5,6,64,67,65,66\"\n",
        "    ),\n",
        "    \"Growth\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=an_recom_buybetter,fa_epsqoq_o20,fa_salesqoq_o20,\"\n",
        "        f\"ind_stocksonly,sh_avgvol_o1000,sh_price_o10,ta_perf_4wup,\"\n",
        "        f\"ta_perf2_13wup,ta_sma20_pa,ta_sma200_pa,ta_sma50_pa&ft=4\"\n",
        "    ),\n",
        "    \"IPO\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=cap_midover,ind_stocksonly,ipodate_prev3yrs,sh_avgvol_o1000,\"\n",
        "        f\"sh_price_o10,ta_beta_o0.5,ta_sma20_pa&ft=4\"\n",
        "    ),\n",
        "    \"52 Week High\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=ind_stocksonly,sh_avgvol_o1000,sh_price_o10,ta_beta_o1,\"\n",
        "        f\"ta_highlow52w_nh&ft=4\"\n",
        "    ),\n",
        "    \"Week 20%+ Gain\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=cap_smallover,ind_stocksonly,sh_avgvol_o1000,ta_perf_1w30o,\"\n",
        "        f\"ta_sma20_pa,ta_volatility_wo4&ft=4&o=-marketcap\"\n",
        "    )\n",
        "}\n",
        "\n",
        "def fetch_all_tickers(screener_url: str, max_pages: int = 10) -> pd.DataFrame:\n",
        "    combined = []\n",
        "    seen = set()\n",
        "    page = 1\n",
        "\n",
        "    while page <= max_pages:\n",
        "        resp = session.get(f\"{screener_url}&r={1+(page-1)*20}\", timeout=10)\n",
        "        if resp.status_code != 200:\n",
        "            break\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "        rows = soup.select('tr[valign=\"top\"]')\n",
        "        if not rows:\n",
        "            break\n",
        "\n",
        "        new_data = False\n",
        "        for row in rows:\n",
        "            cols = row.find_all('td')\n",
        "            if len(cols) == 11:\n",
        "                ticker = cols[1].text.strip()\n",
        "                if ticker and ticker not in seen:\n",
        "                    combined.append([c.text.strip() for c in cols])\n",
        "                    seen.add(ticker)\n",
        "                    new_data = True\n",
        "        if not new_data:\n",
        "            break\n",
        "        page += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "    columns = ['No.', 'Ticker', 'Company', 'Sector', 'Industry',\n",
        "               'Country', 'Market Cap', 'P/E', 'Volume', 'Price', 'Change']\n",
        "    return pd.DataFrame(combined, columns=columns) if combined else pd.DataFrame(columns=columns)\n",
        "\n",
        "def aggregate_and_save(screener_map: dict) -> (pd.DataFrame, str, str):\n",
        "    mapping = defaultdict(list)\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    for name, url in screener_map.items():\n",
        "        df = fetch_all_tickers(url)\n",
        "        print(f\"{name}: {len(df)} tickers found\")\n",
        "        for t in df['Ticker'].unique():\n",
        "            mapping[t].append(name)\n",
        "\n",
        "    if not mapping:\n",
        "        mapping['TSLA'].append('Default Screener')\n",
        "\n",
        "    data = []\n",
        "    for t, screens in mapping.items():\n",
        "        data.append({\n",
        "            'Ticker': t,\n",
        "            'Appearances': len(screens),\n",
        "            'Screeners': \", \".join(screens)\n",
        "        })\n",
        "    summary_df = pd.DataFrame(data).sort_values(['Appearances','Ticker'], ascending=[False, True])\n",
        "\n",
        "    csv_file = f\"finviz_screeners_{today}.csv\"\n",
        "    html_file = f\"finviz_screeners_{today}.html\"\n",
        "    summary_df.to_csv(csv_file, index=False)\n",
        "    summary_df.to_html(html_file, index=False)\n",
        "\n",
        "    return summary_df, csv_file, html_file\n",
        "\n",
        "# ----------------------------\n",
        "# Part 2: Snapshot Fetch with Retries\n",
        "# ----------------------------\n",
        "def get_snapshot_metrics(ticker: str, max_retries: int = 5):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            resp = session.get(f\"{FINVIZ_BASE}/quote.ashx\", params={\"t\": ticker})\n",
        "            resp.raise_for_status()\n",
        "            soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "            table = soup.find(\"table\", class_=\"snapshot-table2\")\n",
        "            if not table:\n",
        "                raise ValueError(\"Snapshot table not found\")\n",
        "\n",
        "            data = {}\n",
        "            for row in table.find_all(\"tr\"):\n",
        "                cells = row.find_all(\"td\")\n",
        "                for key_cell, val_cell in zip(cells[0::2], cells[1::2]):\n",
        "                    key = key_cell.get_text(strip=True).rstrip('.')\n",
        "                    data[key] = val_cell.get_text(strip=True)\n",
        "\n",
        "            atr_pct = float(data.get(\"ATR (14)\", 0)) / float(data.get(\"Price\", \"1\").replace(',', '')) * 100\n",
        "            eps = float(data.get(\"EPS Y/Y TTM\", '0').replace('%',''))\n",
        "            sales = float(data.get(\"Sales Y/Y TTM\", '0').replace('%',''))\n",
        "            return atr_pct, eps, sales\n",
        "\n",
        "        except requests.HTTPError as e:\n",
        "            if e.response.status_code == 429:\n",
        "                wait = (2 ** attempt) + random.random()\n",
        "                print(f\"Rate limited fetching snapshot for {ticker}; retrying in {wait:.1f}s…\")\n",
        "                time.sleep(wait)\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"HTTP error for {ticker}: {e}\")\n",
        "                break\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching snapshot for {ticker}: {e}\")\n",
        "            break\n",
        "\n",
        "    return None, None, None\n",
        "\n",
        "# ----------------------------\n",
        "# Part 3: Chart Gallery\n",
        "# ----------------------------\n",
        "def generate_finviz_gallery(tickers: list) -> str:\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    out_html = f\"finviz_chart_grid_{today}.html\"\n",
        "    html = [\n",
        "        '<html><head><title>Finviz Chart Gallery</title>',\n",
        "        '<style>',\n",
        "        'body { font-family: Arial; background: #f5f5f5; padding: 20px }',\n",
        "        '.chart-grid { display: flex; flex-wrap: wrap; gap: 20px }',\n",
        "        '.chart-item { width: 23%; background: white; border:1px solid #ccc;',\n",
        "        'padding:10px; box-shadow:2px 2px 5px rgba(0,0,0,0.1); text-align:center }',\n",
        "        '.chart-item img { max-width:100%; height:auto }',\n",
        "        'h2 { text-align:center }',\n",
        "        '</style></head><body><h2>Finviz Chart Gallery</h2>',\n",
        "        '<div class=\"chart-grid\">'\n",
        "    ]\n",
        "    for t in tickers:\n",
        "        url = f\"{FINVIZ_BASE}/chart.ashx\"\n",
        "        params = {\"t\": t, \"ty\": \"c\", \"ta\": 1, \"p\": \"d\", \"s\": \"m\"}\n",
        "        req = requests.Request('GET', url, params=params).prepare()\n",
        "        html.append(f'<div class=\"chart-item\"><h4>{t}</h4>'\n",
        "                    f'<img src=\"{req.url}\" alt=\"{t}\"></div>')\n",
        "    html.append('</div></body></html>')\n",
        "\n",
        "    with open(out_html, 'w') as f:\n",
        "        f.write(\"\\n\".join(html))\n",
        "    return out_html\n",
        "\n",
        "# ----------------------------\n",
        "# Part 4: Main Execution with Logging\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    summary_df, csv_path, html_summary = aggregate_and_save(screener_urls)\n",
        "    print(f\"Summary CSV: {csv_path}\\nSummary HTML: {html_summary}\")\n",
        "\n",
        "    print(f\"Initial tickers count: {len(summary_df)}\")\n",
        "\n",
        "    # Fetch snapshot metrics\n",
        "    summary_df[['ATR%', 'EPS Y/Y TTM', 'Sales Y/Y TTM']] = summary_df['Ticker'].apply(\n",
        "        lambda t: pd.Series(get_snapshot_metrics(t))\n",
        "    )\n",
        "\n",
        "    filter_a = summary_df[summary_df['ATR%'] > 3.0]\n",
        "    print(f\"Tickers with ATR% > 3.0: {len(filter_a)}\")\n",
        "\n",
        "    # filter_b = filter_a[\n",
        "    #     (filter_a['EPS Y/Y TTM'] > 20) &\n",
        "    #     (filter_a['Sales Y/Y TTM'] > 20)\n",
        "    # ]\n",
        "    # print(f\"Tickers with ATR% > 3.0 AND EPS & Sales Y/Y TTM > 20: {len(filter_b)}\")\n",
        "\n",
        "    # today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    # filter_b.to_csv(f\"finviz_filtered_{today}.csv\", index=False)\n",
        "    # print(f\"Filtered results saved: finviz_filtered_{today}.csv\")\n",
        "\n",
        "    gallery_path = generate_finviz_gallery(filter_a['Ticker'].tolist())\n",
        "    print(f\"Filtered chart gallery HTML: {gallery_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYif8TdFM_Bx",
        "outputId": "c109ce09-f472-4954-cec5-d02940cfe2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10% Change: 10 tickers found\n",
            "Growth: 42 tickers found\n",
            "IPO: 26 tickers found\n",
            "52 Week High: 32 tickers found\n",
            "Week 20%+ Gain: 56 tickers found\n",
            "Summary CSV: finviz_screeners_2025-05-14.csv\n",
            "Summary HTML: finviz_screeners_2025-05-14.html\n",
            "Initial tickers count: 143\n",
            "Error fetching snapshot for ACHR: could not convert string to float: '-'\n",
            "Error fetching snapshot for AHR: could not convert string to float: '-'\n",
            "Rate limited fetching snapshot for AAOI; retrying in 1.3s…\n",
            "Rate limited fetching snapshot for BCRX; retrying in 1.4s…\n",
            "Rate limited fetching snapshot for BMBL; retrying in 1.3s…\n",
            "Error fetching snapshot for BTDR: could not convert string to float: '-'\n",
            "Rate limited fetching snapshot for ELAN; retrying in 2.0s…\n",
            "Rate limited fetching snapshot for HSAI; retrying in 1.3s…\n",
            "Rate limited fetching snapshot for NXT; retrying in 1.4s…\n",
            "Error fetching snapshot for RYET: could not convert string to float: '-'\n",
            "Rate limited fetching snapshot for SHLS; retrying in 1.6s…\n",
            "Error fetching snapshot for SOC: could not convert string to float: '-'\n",
            "Rate limited fetching snapshot for VIK; retrying in 1.8s…\n",
            "Tickers with ATR% > 3.0: 125\n",
            "Filtered chart gallery HTML: finviz_chart_grid_2025-05-14.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Imports & Setup\n",
        "# ----------------------------\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "import random\n",
        "\n",
        "FINVIZ_BASE = \"https://finviz.com\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    )\n",
        "}\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update(HEADERS)\n",
        "\n",
        "# ----------------------------\n",
        "# Part 1: Screener Fetch & Save\n",
        "# ----------------------------\n",
        "screener_urls = {\n",
        "    \"10% Change\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=151\"\n",
        "        f\"&f=ind_stocksonly,sh_avgvol_o500,sh_price_o5,ta_changeopen_u10,\"\n",
        "        f\"ta_sma20_sa50,ta_sma50_pa&ft=4&o=-relativevolume&\"\n",
        "        f\"c=0,1,2,3,4,5,6,64,67,65,66\"\n",
        "    ),\n",
        "    \"Growth\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=an_recom_buybetter,fa_epsqoq_o20,fa_salesqoq_o20,\"\n",
        "        f\"ind_stocksonly,sh_avgvol_o1000,sh_price_o10,ta_perf_4wup,\"\n",
        "        f\"ta_perf2_13wup,ta_sma20_pa,ta_sma200_pa,ta_sma50_pa&ft=4\"\n",
        "    ),\n",
        "    \"IPO\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=cap_midover,ind_stocksonly,ipodate_prev3yrs,sh_avgvol_o1000,\"\n",
        "        f\"sh_price_o10,ta_beta_o0.5,ta_sma20_pa&ft=4\"\n",
        "    ),\n",
        "    \"52 Week High\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=ind_stocksonly,sh_avgvol_o1000,sh_price_o10,ta_beta_o1,\"\n",
        "        f\"ta_highlow52w_nh&ft=4\"\n",
        "    ),\n",
        "    \"Week 20%+ Gain\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=cap_smallover,ind_stocksonly,sh_avgvol_o1000,ta_perf_1w30o,\"\n",
        "        f\"ta_sma20_pa,ta_volatility_wo4&ft=4&o=-marketcap\"\n",
        "    )\n",
        "}\n",
        "\n",
        "def fetch_all_tickers(screener_url: str, max_pages: int = 10) -> pd.DataFrame:\n",
        "    combined = []\n",
        "    seen = set()\n",
        "    page = 1\n",
        "\n",
        "    while page <= max_pages:\n",
        "        resp = session.get(f\"{screener_url}&r={1+(page-1)*20}\", timeout=10)\n",
        "        if resp.status_code != 200:\n",
        "            break\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "        rows = soup.select('tr[valign=\"top\"]')\n",
        "        if not rows:\n",
        "            break\n",
        "\n",
        "        new_data = False\n",
        "        for row in rows:\n",
        "            cols = row.find_all('td')\n",
        "            if len(cols) == 11:\n",
        "                ticker = cols[1].text.strip()\n",
        "                if ticker and ticker not in seen:\n",
        "                    combined.append([c.text.strip() for c in cols])\n",
        "                    seen.add(ticker)\n",
        "                    new_data = True\n",
        "        if not new_data:\n",
        "            break\n",
        "        page += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "    columns = ['No.', 'Ticker', 'Company', 'Sector', 'Industry',\n",
        "               'Country', 'Market Cap', 'P/E', 'Volume', 'Price', 'Change']\n",
        "    return pd.DataFrame(combined, columns=columns) if combined else pd.DataFrame(columns=columns)\n",
        "\n",
        "def aggregate_and_save(screener_map: dict) -> (pd.DataFrame, str, str):\n",
        "    mapping = defaultdict(list)\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    for name, url in screener_map.items():\n",
        "        df = fetch_all_tickers(url)\n",
        "        print(f\"{name}: {len(df)} tickers found\")\n",
        "        for t in df['Ticker'].unique():\n",
        "            mapping[t].append(name)\n",
        "\n",
        "    if not mapping:\n",
        "        mapping['TSLA'].append('Default Screener')\n",
        "\n",
        "    data = []\n",
        "    for t, screens in mapping.items():\n",
        "        data.append({\n",
        "            'Ticker': t,\n",
        "            'Appearances': len(screens),\n",
        "            'Screeners': \", \".join(screens)\n",
        "        })\n",
        "    summary_df = pd.DataFrame(data).sort_values(['Appearances','Ticker'], ascending=[False, True])\n",
        "\n",
        "    csv_file = f\"finviz_screeners_{today}.csv\"\n",
        "    html_file = f\"finviz_screeners_{today}.html\"\n",
        "    summary_df.to_csv(csv_file, index=False)\n",
        "    summary_df.to_html(html_file, index=False)\n",
        "\n",
        "    return summary_df, csv_file, html_file\n",
        "\n",
        "# ----------------------------\n",
        "# Part 2: Snapshot Fetch with Retries\n",
        "# ----------------------------\n",
        "def get_snapshot_metrics(ticker: str, max_retries: int = 5):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            resp = session.get(f\"{FINVIZ_BASE}/quote.ashx\", params={\"t\": ticker})\n",
        "            resp.raise_for_status()\n",
        "            soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "            table = soup.find(\"table\", class_=\"snapshot-table2\")\n",
        "            if not table:\n",
        "                raise ValueError(\"Snapshot table not found\")\n",
        "\n",
        "            data = {}\n",
        "            for row in table.find_all(\"tr\"):\n",
        "                cells = row.find_all(\"td\")\n",
        "                for key_cell, val_cell in zip(cells[0::2], cells[1::2]):\n",
        "                    key = key_cell.get_text(strip=True).rstrip('.')\n",
        "                    data[key] = val_cell.get_text(strip=True)\n",
        "\n",
        "            atr_pct = float(data.get(\"ATR (14)\", 0)) / float(data.get(\"Price\", \"1\").replace(',', '')) * 100\n",
        "            eps = float(data.get(\"EPS Y/Y TTM\", '0').replace('%',''))\n",
        "            sales = float(data.get(\"Sales Y/Y TTM\", '0').replace('%',''))\n",
        "            return atr_pct, eps, sales\n",
        "\n",
        "        except requests.HTTPError as e:\n",
        "            if e.response.status_code == 429:\n",
        "                wait = (2 ** attempt) + random.random()\n",
        "                print(f\"Rate limited fetching snapshot for {ticker}; retrying in {wait:.1f}s…\")\n",
        "                time.sleep(wait)\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"HTTP error for {ticker}: {e}\")\n",
        "                break\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching snapshot for {ticker}: {e}\")\n",
        "            break\n",
        "\n",
        "    return None, None, None\n",
        "\n",
        "# ----------------------------\n",
        "# Part 3: Chart Gallery\n",
        "# ----------------------------\n",
        "def generate_finviz_gallery(tickers: list) -> str:\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    out_html = f\"finviz_chart_grid_{today}.html\"\n",
        "    html = [\n",
        "        '<html><head><title>Finviz Chart Gallery</title>',\n",
        "        '<style>',\n",
        "        'body { font-family: Arial; background: #f5f5f5; padding: 20px }',\n",
        "        '.chart-grid { display: flex; flex-wrap: wrap; gap: 20px }',\n",
        "        '.chart-item { width: 23%; background: white; border:1px solid #ccc;',\n",
        "        'padding:10px; box-shadow:2px 2px 5px rgba(0,0,0,0.1); text-align:center }',\n",
        "        '.chart-item img { max-width:100%; height:auto }',\n",
        "        'h2 { text-align:center }',\n",
        "        '</style></head><body><h2>Finviz Chart Gallery</h2>',\n",
        "        '<div class=\"chart-grid\">'\n",
        "    ]\n",
        "    for t in tickers:\n",
        "        url = f\"{FINVIZ_BASE}/chart.ashx\"\n",
        "        params = {\"t\": t, \"ty\": \"c\", \"ta\": 1, \"p\": \"d\", \"s\": \"m\"}\n",
        "        req = requests.Request('GET', url, params=params).prepare()\n",
        "        html.append(f'<div class=\"chart-item\"><h4>{t}</h4>'\n",
        "                    f'<img src=\"{req.url}\" alt=\"{t}\"></div>')\n",
        "    html.append('</div></body></html>')\n",
        "\n",
        "    with open(out_html, 'w') as f:\n",
        "        f.write(\"\\n\".join(html))\n",
        "    return out_html\n",
        "\n",
        "# ----------------------------\n",
        "# Part 4: Main Execution with Logging\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    summary_df, csv_path, html_summary = aggregate_and_save(screener_urls)\n",
        "    print(f\"Summary CSV: {csv_path}\\nSummary HTML: {html_summary}\")\n",
        "\n",
        "    print(f\"Initial tickers count: {len(summary_df)}\")\n",
        "\n",
        "    # Fetch snapshot metrics\n",
        "    summary_df[['ATR%', 'EPS Y/Y TTM', 'Sales Y/Y TTM']] = summary_df['Ticker'].apply(\n",
        "        lambda t: pd.Series(get_snapshot_metrics(t))\n",
        "    )\n",
        "\n",
        "    filter_a = summary_df[summary_df['ATR%'] > 3.0]\n",
        "    print(f\"Tickers with ATR% > 3.0: {len(filter_a)}\")\n",
        "\n",
        "    filter_b = filter_a[\n",
        "        (filter_a['EPS Y/Y TTM'] > 20) &\n",
        "        (filter_a['Sales Y/Y TTM'] > 20)\n",
        "    ]\n",
        "    print(f\"Tickers with ATR% > 3.0 AND EPS & Sales Y/Y TTM > 20: {len(filter_b)}\")\n",
        "\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    filter_b.to_csv(f\"finviz_filtered_{today}.csv\", index=False)\n",
        "    print(f\"Filtered results saved: finviz_filtered_{today}.csv\")\n",
        "\n",
        "    gallery_path = generate_finviz_gallery(filter_b['Ticker'].tolist())\n",
        "    print(f\"Filtered chart gallery HTML: {gallery_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTKNbedu97Y7",
        "outputId": "82ca01be-c4a4-498b-acf3-11ca44320e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10% Change: 10 tickers found\n",
            "Growth: 42 tickers found\n",
            "IPO: 26 tickers found\n",
            "52 Week High: 32 tickers found\n",
            "Week 20%+ Gain: 56 tickers found\n",
            "Summary CSV: finviz_screeners_2025-05-14.csv\n",
            "Summary HTML: finviz_screeners_2025-05-14.html\n",
            "Initial tickers count: 143\n",
            "Error fetching snapshot for ACHR: could not convert string to float: '-'\n",
            "Error fetching snapshot for AHR: could not convert string to float: '-'\n",
            "Rate limited fetching snapshot for AAOI; retrying in 1.5s…\n",
            "Rate limited fetching snapshot for BROS; retrying in 1.1s…\n",
            "Rate limited fetching snapshot for BROS; retrying in 2.1s…\n",
            "Error fetching snapshot for BTDR: could not convert string to float: '-'\n",
            "Rate limited fetching snapshot for ENVX; retrying in 1.7s…\n",
            "Rate limited fetching snapshot for JCI; retrying in 1.3s…\n",
            "Rate limited fetching snapshot for JCI; retrying in 2.3s…\n",
            "Rate limited fetching snapshot for RUM; retrying in 1.9s…\n",
            "Error fetching snapshot for RYET: could not convert string to float: '-'\n",
            "Error fetching snapshot for SOC: could not convert string to float: '-'\n",
            "Rate limited fetching snapshot for TLN; retrying in 1.1s…\n",
            "Rate limited fetching snapshot for TLN; retrying in 2.8s…\n",
            "Tickers with ATR% > 3.0: 125\n",
            "Tickers with ATR% > 3.0 AND EPS & Sales Y/Y TTM > 20: 41\n",
            "Filtered results saved: finviz_filtered_2025-05-14.csv\n",
            "Filtered chart gallery HTML: finviz_chart_grid_2025-05-14.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "import random\n",
        "\n",
        "# ----------------------------\n",
        "# Constants & Shared Session\n",
        "# ----------------------------\n",
        "FINVIZ_BASE = \"https://finviz.com\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    )\n",
        "}\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update(HEADERS)\n",
        "\n",
        "# ----------------------------\n",
        "# Part 1: Screener Fetch & Save\n",
        "# ----------------------------\n",
        "\n",
        "screener_urls = {\n",
        "    \"10% Change\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=151\"\n",
        "        f\"&f=ind_stocksonly,sh_avgvol_o500,sh_price_o5,ta_changeopen_u10,\"\n",
        "        f\"ta_sma20_sa50,ta_sma50_pa&ft=4&o=-relativevolume&\"\n",
        "        f\"c=0,1,2,3,4,5,6,64,67,65,66\"\n",
        "    ),\n",
        "    \"Growth\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=an_recom_buybetter,fa_epsqoq_o20,fa_salesqoq_o20,\"\n",
        "        f\"ind_stocksonly,sh_avgvol_o1000,sh_price_o10,ta_perf_4wup,\"\n",
        "        f\"ta_perf2_13wup,ta_sma20_pa,ta_sma200_pa,ta_sma50_pa&ft=4\"\n",
        "    ),\n",
        "    \"IPO\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=ind_stocksonly,ipodate_prev2yrs,sh_avgvol_o1000,sh_price_o10,\"\n",
        "        f\"ta_sma20_pa&ft=4\"\n",
        "    ),\n",
        "    \"52 Week High\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=sh_avgvol_o1000,sh_price_o10,ta_beta_o1,ta_highlow52w_nh&ft=4\"\n",
        "    ),\n",
        "    \"Week 20%+ Gain\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=cap_smallover,sh_avgvol_o1000,sh_price_o3,ta_perf_1w20o,\"\n",
        "        f\"ta_volatility_wo4&ft=4&o=-marketcap&r=25\"\n",
        "    )\n",
        "}\n",
        "\n",
        "\n",
        "def fetch_all_tickers(screener_url: str, max_pages: int = 10) -> pd.DataFrame:\n",
        "    combined = []\n",
        "    seen = set()\n",
        "    page = 1\n",
        "\n",
        "    while page <= max_pages:\n",
        "        resp = session.get(f\"{screener_url}&r={1+(page-1)*20}\", timeout=10)\n",
        "        if resp.status_code != 200:\n",
        "            break\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "        rows = soup.select('tr[valign=\"top\"]')\n",
        "        if not rows:\n",
        "            break\n",
        "\n",
        "        new_data = False\n",
        "        for row in rows:\n",
        "            cols = row.find_all('td')\n",
        "            if len(cols) == 11:\n",
        "                ticker = cols[1].text.strip()\n",
        "                if ticker and ticker not in seen:\n",
        "                    combined.append([c.text.strip() for c in cols])\n",
        "                    seen.add(ticker)\n",
        "                    new_data = True\n",
        "        if not new_data:\n",
        "            break\n",
        "        page += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "    columns = ['No.', 'Ticker', 'Company', 'Sector', 'Industry',\n",
        "               'Country', 'Market Cap', 'P/E', 'Volume', 'Price', 'Change']\n",
        "    return pd.DataFrame(combined, columns=columns) if combined else pd.DataFrame(columns=columns)\n",
        "\n",
        "\n",
        "def aggregate_and_save(screener_map: dict) -> (pd.DataFrame, str, str):\n",
        "    mapping = defaultdict(list)\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    for name, url in screener_map.items():\n",
        "        df = fetch_all_tickers(url)\n",
        "        print(f\"{name}: {len(df)} tickers found\")\n",
        "        for t in df['Ticker'].unique():\n",
        "            mapping[t].append(name)\n",
        "\n",
        "    if not mapping:\n",
        "        mapping['TSLA'].append('Default Screener')\n",
        "\n",
        "    data = []\n",
        "    for t, screens in mapping.items():\n",
        "        data.append({\n",
        "            'Ticker': t,\n",
        "            'Appearances': len(screens),\n",
        "            'Screeners': \", \".join(screens)\n",
        "        })\n",
        "    summary_df = pd.DataFrame(data).sort_values(['Appearances','Ticker'], ascending=[False, True])\n",
        "\n",
        "    csv_file = f\"finviz_screeners_{today}.csv\"\n",
        "    html_file = f\"finviz_screeners_{today}.html\"\n",
        "    summary_df.to_csv(csv_file, index=False)\n",
        "    summary_df.to_html(html_file, index=False)\n",
        "\n",
        "    return summary_df, csv_file, html_file\n",
        "\n",
        "# ----------------------------\n",
        "# Part 2: ATR% Scraper & Filter\n",
        "# ----------------------------\n",
        "\n",
        "def get_atr_pct(ticker: str, max_retries: int = 5) -> float:\n",
        "    \"\"\"\n",
        "    Fetches ATR% using current Price field with retries on HTTP 429.\n",
        "    \"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            resp = session.get(f\"{FINVIZ_BASE}/quote.ashx\", params={\"t\": ticker})\n",
        "            resp.raise_for_status()\n",
        "            soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "            table = soup.find(\"table\", class_=\"snapshot-table2\")\n",
        "            if not table:\n",
        "                raise ValueError(\"Snapshot table not found\")\n",
        "\n",
        "            data = {}\n",
        "            for row in table.find_all(\"tr\"):\n",
        "                cells = row.find_all(\"td\")\n",
        "                for key_cell, val_cell in zip(cells[0::2], cells[1::2]):\n",
        "                    key = key_cell.get_text(strip=True).rstrip('.')\n",
        "                    data[key] = val_cell.get_text(strip=True)\n",
        "\n",
        "            atr_str   = data.get(\"ATR (14)\")\n",
        "            price_str = data.get(\"Price\")\n",
        "            eps_str = data.get(\"EPS Y/Y TTM\")\n",
        "            sales_str = data.get(\"Sales Y/Y TTM\")\n",
        "\n",
        "            if not atr_str or not price_str:\n",
        "                raise ValueError(\"Missing ATR or Price\")\n",
        "\n",
        "            atr   = float(atr_str)\n",
        "            price = float(price_str.replace(',',''))\n",
        "            eps = float(eps_str)\n",
        "            sales = float(sales_str)\n",
        "            return (atr / price) * 100\n",
        "\n",
        "        except requests.HTTPError as e:\n",
        "            if e.response.status_code == 429:\n",
        "                wait = (2 ** attempt) + random.random()\n",
        "                print(f\"Rate limited fetching ATR% for {ticker}; retrying in {wait:.1f}s…\")\n",
        "                time.sleep(wait)\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"HTTP error for {ticker}: {e}\")\n",
        "                break\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing ATR% for {ticker}: {e}\")\n",
        "            break\n",
        "\n",
        "    print(f\"⚠️ Giving up on ATR% for {ticker}\")\n",
        "    return None\n",
        "\n",
        "# ----------------------------\n",
        "# Part 3: Generate Chart Gallery with Date\n",
        "# ----------------------------\n",
        "\n",
        "def generate_finviz_gallery(tickers: list) -> str:\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    out_html = f\"finviz_chart_grid_{today}.html\"\n",
        "    html = [\n",
        "        '<html><head><title>Finviz Chart Gallery</title>',\n",
        "        '<style>',\n",
        "        'body { font-family: Arial; background: #f5f5f5; padding: 20px }',\n",
        "        '.chart-grid { display: flex; flex-wrap: wrap; gap: 20px }',\n",
        "        '.chart-item { width: 23%; background: white; border:1px solid #ccc;',\n",
        "        'padding:10px; box-shadow:2px 2px 5px rgba(0,0,0,0.1); text-align:center }',\n",
        "        '.chart-item img { max-width:100%; height:auto }',\n",
        "        'h2 { text-align:center }',\n",
        "        '</style></head><body><h2>Finviz Chart Gallery</h2>',\n",
        "        '<div class=\"chart-grid\">'\n",
        "    ]\n",
        "    for t in tickers:\n",
        "        url = f\"{FINVIZ_BASE}/chart.ashx\"\n",
        "        params = {\"t\": t, \"ty\": \"c\", \"ta\": 1, \"p\": \"d\", \"s\": \"m\"}\n",
        "        req = requests.Request('GET', url, params=params).prepare()\n",
        "        html.append(f'<div class=\"chart-item\"><h4>{t}</h4>'\n",
        "                    f'<img src=\"{req.url}\" alt=\"{t}\"></div>')\n",
        "    html.append('</div></body></html>')\n",
        "\n",
        "    with open(out_html, 'w') as f:\n",
        "        f.write(\"\\n\".join(html))\n",
        "    return out_html\n",
        "\n",
        "# ----------------------------\n",
        "# Main Execution\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Part 1: Summary\n",
        "    summary_df, csv_path, html_summary = aggregate_and_save(screener_urls)\n",
        "    print(f\"Summary CSV: {csv_path}\\nSummary HTML: {html_summary}\")\n",
        "\n",
        "    # Part 2: ATR% & Filtering\n",
        "    total = len(summary_df)\n",
        "    summary_df['ATR%'] = summary_df['Ticker'].apply(get_atr_pct)\n",
        "    available = summary_df['ATR%'].notnull().sum()\n",
        "    filtered = summary_df[summary_df['ATR%'] > 3.0]\n",
        "    print(f\"Total tickers: {total}\")\n",
        "    print(f\"Tickers with ATR% data: {available}\")\n",
        "    print(f\"Tickers > 3% ATR%: {len(filtered)}\")\n",
        "\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    filtered.to_csv(f\"finviz_screeners_filtered_{today}.csv\", index=False)\n",
        "    filtered.to_html(f\"finviz_screeners_filtered_{today}.html\", index=False)\n",
        "    print(f\"Filtered CSV/HTML written for ATR% > 3%\")\n",
        "\n",
        "    # Part 3: Gallery for filtered tickers\n",
        "    gallery_path = generate_finviz_gallery(filtered['Ticker'].tolist())\n",
        "    print(f\"Filtered chart gallery HTML: {gallery_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UMTS6On9OVj",
        "outputId": "abeb9e50-a08e-44d2-ad20-bd0caea41a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10% Change: 13 tickers found\n",
            "Growth: 34 tickers found\n",
            "IPO: 46 tickers found\n",
            "52 Week High: 11 tickers found\n",
            "Week 20%+ Gain: 37 tickers found\n",
            "Summary CSV: finviz_screeners_2025-05-11.csv\n",
            "Summary HTML: finviz_screeners_2025-05-11.html\n",
            "Rate limited fetching ATR% for ARM; retrying in 1.7s…\n",
            "Rate limited fetching ATR% for CRWV; retrying in 1.6s…\n",
            "Rate limited fetching ATR% for GOGO; retrying in 1.5s…\n",
            "Rate limited fetching ATR% for OS; retrying in 1.0s…\n",
            "Rate limited fetching ATR% for SNDK; retrying in 1.6s…\n",
            "Total tickers: 125\n",
            "Tickers with ATR% data: 125\n",
            "Tickers > 3% ATR%: 105\n",
            "Filtered CSV/HTML written for ATR% > 3%\n",
            "Filtered chart gallery HTML: finviz_chart_grid_2025-05-11.html\n",
            "CPU times: user 20.3 s, sys: 148 ms, total: 20.5 s\n",
            "Wall time: 53.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "# ----------------------------\n",
        "# Constants & Shared Session\n",
        "# ----------------------------\n",
        "FINVIZ_BASE = \"https://finviz.com\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    )\n",
        "}\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update(HEADERS)\n",
        "\n",
        "# ----------------------------\n",
        "# Part 1: Screener Fetch & Save\n",
        "# ----------------------------\n",
        "\n",
        "# Screener URLs\n",
        "screener_urls = {\n",
        "    \"10% Change\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=151\"\n",
        "        f\"&f=ind_stocksonly,sh_avgvol_o500,sh_price_o5,ta_changeopen_u10,\"\n",
        "        f\"ta_sma20_sa50,ta_sma50_pa&ft=4&o=-relativevolume&\"\n",
        "        f\"c=0,1,2,3,4,5,6,64,67,65,66\"\n",
        "    ),\n",
        "    \"Growth\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=an_recom_buybetter,fa_epsqoq_o20,fa_salesqoq_o20,\"\n",
        "        f\"ind_stocksonly,sh_avgvol_o1000,sh_price_o10,ta_perf_4wup,\"\n",
        "        f\"ta_perf2_13wup,ta_sma20_pa,ta_sma200_pa,ta_sma50_pa&ft=4\"\n",
        "    ),\n",
        "    \"IPO\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=ind_stocksonly,ipodate_prev2yrs,sh_avgvol_o1000,sh_price_o10,\"\n",
        "        f\"ta_sma20_pa&ft=4\"\n",
        "    ),\n",
        "    \"52 Week High\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=sh_avgvol_o1000,sh_price_o10,ta_beta_o1,ta_highlow52w_nh&ft=4\"\n",
        "    ),\n",
        "    \"Week 20%+ Gain\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=cap_smallover,sh_avgvol_o1000,sh_price_o3,ta_perf_1w20o,\"\n",
        "        f\"ta_volatility_wo4&ft=4&o=-marketcap&r=25\"\n",
        "    )\n",
        "}\n",
        "\n",
        "def fetch_all_tickers(screener_url: str, max_pages: int = 10) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch tickers from a Finviz screener URL, paginating up to max_pages.\n",
        "    Returns DataFrame with columns: No., Ticker, Company, Sector, Industry,\n",
        "    Country, Market Cap, P/E, Volume, Price, Change.\n",
        "    \"\"\"\n",
        "    combined = []\n",
        "    seen = set()\n",
        "    page = 1\n",
        "\n",
        "    while page <= max_pages:\n",
        "        resp = session.get(f\"{screener_url}&r={1+(page-1)*20}\", timeout=10)\n",
        "        if resp.status_code != 200:\n",
        "            break\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "        rows = soup.select('tr[valign=\"top\"]')\n",
        "        if not rows:\n",
        "            break\n",
        "\n",
        "        new_data = False\n",
        "        for row in rows:\n",
        "            cols = row.find_all('td')\n",
        "            if len(cols) == 11:\n",
        "                ticker = cols[1].text.strip()\n",
        "                if ticker and ticker not in seen:\n",
        "                    combined.append([c.text.strip() for c in cols])\n",
        "                    seen.add(ticker)\n",
        "                    new_data = True\n",
        "        if not new_data:\n",
        "            break\n",
        "        page += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "    columns = ['No.', 'Ticker', 'Company', 'Sector', 'Industry',\n",
        "               'Country', 'Market Cap', 'P/E', 'Volume', 'Price', 'Change']\n",
        "    return pd.DataFrame(combined, columns=columns) if combined else pd.DataFrame(columns=columns)\n",
        "\n",
        "\n",
        "def aggregate_and_save(screener_map: dict) -> (pd.DataFrame, str, str):\n",
        "    \"\"\"\n",
        "    Aggregates tickers across screeners, saves CSV & HTML summary.\n",
        "    Returns (DataFrame, csv_path, html_path).\n",
        "    \"\"\"\n",
        "    mapping = defaultdict(list)\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    for name, url in screener_map.items():\n",
        "        df = fetch_all_tickers(url)\n",
        "        print(f\"{name}: {len(df)} tickers found\")\n",
        "        for t in df['Ticker'].unique():\n",
        "            mapping[t].append(name)\n",
        "\n",
        "    if not mapping:\n",
        "        mapping['TSLA'].append('Default Screener')\n",
        "\n",
        "    data = []\n",
        "    for t, screens in mapping.items():\n",
        "        data.append({\n",
        "            'Ticker': t,\n",
        "            'Appearances': len(screens),\n",
        "            'Screeners': \", \".join(screens)\n",
        "        })\n",
        "    summary_df = pd.DataFrame(data).sort_values(['Appearances','Ticker'], ascending=[False, True])\n",
        "\n",
        "    csv_file = f\"finviz_screeners_{today}.csv\"\n",
        "    html_file = f\"finviz_screeners_{today}.html\"\n",
        "    summary_df.to_csv(csv_file, index=False)\n",
        "    summary_df.to_html(html_file, index=False)\n",
        "\n",
        "    return summary_df, csv_file, html_file\n",
        "\n",
        "# ----------------------------\n",
        "# Part 2: ATR% Scraper & Filter\n",
        "# ----------------------------\n",
        "\n",
        "def get_atr_pct(ticker: str, max_retries: int = 3) -> float:\n",
        "    \"\"\"\n",
        "    Scrapes Finviz for ATR (14) and Prev Close, returns ATR% or None.\n",
        "    Retries on 429 errors with exponential back-off.\n",
        "    \"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            resp = session.get(f\"{FINVIZ_BASE}/quote.ashx\", params={\"t\": ticker}, timeout=10)\n",
        "            resp.raise_for_status()\n",
        "            soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "            table = soup.find(\"table\", class_=\"snapshot-table2\")\n",
        "            if not table:\n",
        "                return None\n",
        "            data = {}\n",
        "            for row in table.find_all(\"tr\"):\n",
        "                cells = row.find_all(\"td\")\n",
        "                for key_cell, val_cell in zip(cells[0::2], cells[1::2]):\n",
        "                    data[key_cell.get_text(strip=True)] = val_cell.get_text(strip=True)\n",
        "\n",
        "            atr_str   = data.get(\"ATR\")\n",
        "            price_str = data.get(\"Price\")  # use current Price field\n",
        "            if not atr_str or not price_str:\n",
        "                raise ValueError(\"Missing ATR or Price\")\n",
        "\n",
        "            atr   = float(atr_str)\n",
        "            price = float(price_str.replace(',',''))\n",
        "            return (atr / price) * 100\n",
        "\n",
        "\n",
        "        except requests.HTTPError as e:\n",
        "            if resp.status_code == 429:\n",
        "                wait = 2 ** attempt\n",
        "                print(f\"Rate limited fetching ATR% for {ticker}; retrying in {wait}s...\")\n",
        "                time.sleep(wait)\n",
        "                continue\n",
        "            return None\n",
        "        except Exception:\n",
        "            return None\n",
        "    print(f\"Skipping {ticker} after {max_retries} retries due to rate limits.\")\n",
        "    return None\n",
        "\n",
        "# ----------------------------\n",
        "# Part 3: Chart Gallery\n",
        "# ----------------------------\n",
        "\n",
        "def generate_finviz_gallery(tickers: list) -> str:\n",
        "    \"\"\"\n",
        "    Creates HTML displaying a 4-col grid of Finviz charts, dated filename.\n",
        "    \"\"\"\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    out_html = f\"finviz_chart_grid_{today}.html\"\n",
        "    html = [\n",
        "        '<html><head><title>Finviz Chart Gallery</title>',\n",
        "        '<style>',\n",
        "        'body { font-family: Arial; background: #f5f5f5; padding: 20px }',\n",
        "        '.chart-grid { display: flex; flex-wrap: wrap; gap: 20px }',\n",
        "        '.chart-item { width: 23%; background: white; border:1px solid #ccc; ',\n",
        "        'padding:10px; box-shadow:2px 2px 5px rgba(0,0,0,0.1); text-align:center }',\n",
        "        '.chart-item img { max-width:100%; height:auto }',\n",
        "        'h2 { text-align:center }',\n",
        "        '</style></head><body><h2>Finviz Chart Gallery</h2>',\n",
        "        '<div class=\"chart-grid\">'\n",
        "    ]\n",
        "    for t in tickers:\n",
        "        params = {'t': t, 'ty': 'c', 'ta': '1', 'p': 'd', 's': 'm'}\n",
        "        img_src = requests.Request('GET', f\"{FINVIZ_BASE}/chart.ashx\", params=params).prepare().url\n",
        "        html.append(f'<div class=\"chart-item\"><h4>{t}</h4>'\n",
        "                    f'<img src=\"{img_src}\" alt=\"{t}\"></div>')\n",
        "    html.append('</div></body></html>')\n",
        "    with open(out_html, 'w') as f:\n",
        "        f.write(\"\\n\".join(html))\n",
        "    return out_html\n",
        "\n",
        "# ----------------------------\n",
        "# Main Execution\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Aggregate screeners\n",
        "    summary_df, csv_path, html_summary = aggregate_and_save(screener_urls)\n",
        "    total = len(summary_df)\n",
        "    print(f\"Total tickers before ATR% filter: {total}\")\n",
        "\n",
        "    # 2. Compute ATR% and filter >3%\n",
        "    summary_df['ATR%'] = summary_df['Ticker'].apply(lambda t: get_atr_pct(t))\n",
        "    filtered_df = summary_df[summary_df['ATR%'] > 3.0]\n",
        "    filtered_count = len(filtered_df)\n",
        "    print(f\"Tickers with ATR% > 3%: {filtered_count}\")\n",
        "\n",
        "    # 3. Save filtered results\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    filtered_csv = f\"finviz_screeners_filtered_{today}.csv\"\n",
        "    filtered_html = f\"finviz_screeners_filtered_{today}.html\"\n",
        "    filtered_df.to_csv(filtered_csv, index=False)\n",
        "    filtered_df.to_html(filtered_html, index=False)\n",
        "    print(f\"Filtered CSV: {filtered_csv}\\nFiltered HTML: {filtered_html}\")\n",
        "\n",
        "    # 4. Generate gallery for filtered tickers\n",
        "    gallery_path = generate_finviz_gallery(filtered_df['Ticker'].tolist())\n",
        "    print(f\"Filtered chart gallery HTML: {gallery_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZwMJ1mC4gEJ",
        "outputId": "96f93dd4-dc23-4890-cc74-e0ad0a34835e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10% Change: 13 tickers found\n",
            "Growth: 34 tickers found\n",
            "IPO: 46 tickers found\n",
            "52 Week High: 11 tickers found\n",
            "Week 20%+ Gain: 37 tickers found\n",
            "Total tickers before ATR% filter: 125\n",
            "Rate limited fetching ATR% for ARM; retrying in 1s...\n",
            "Rate limited fetching ATR% for CPRX; retrying in 1s...\n",
            "Rate limited fetching ATR% for GDOT; retrying in 1s...\n",
            "Rate limited fetching ATR% for MRP; retrying in 1s...\n",
            "Rate limited fetching ATR% for SE; retrying in 1s...\n",
            "Rate limited fetching ATR% for TACOU; retrying in 1s...\n",
            "Rate limited fetching ATR% for VIK; retrying in 1s...\n",
            "Tickers with ATR% > 3%: 0\n",
            "Filtered CSV: finviz_screeners_filtered_2025-05-11.csv\n",
            "Filtered HTML: finviz_screeners_filtered_2025-05-11.html\n",
            "Filtered chart gallery HTML: finviz_chart_grid_2025-05-11.html\n",
            "CPU times: user 19.5 s, sys: 137 ms, total: 19.6 s\n",
            "Wall time: 51.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "# ----------------------------\n",
        "# Constants & Shared Session\n",
        "# ----------------------------\n",
        "FINVIZ_BASE = \"https://finviz.com\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    )\n",
        "}\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update(HEADERS)\n",
        "\n",
        "# ----------------------------\n",
        "# Part 1: Screener Fetch & Save\n",
        "# ----------------------------\n",
        "\n",
        "# Screener URLs\n",
        "screener_urls = {\n",
        "    \"10% Change\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=151\"\n",
        "        f\"&f=ind_stocksonly,sh_avgvol_o500,sh_price_o5,ta_changeopen_u10,\"\n",
        "        f\"ta_sma20_sa50,ta_sma50_pa&ft=4&o=-relativevolume&\"\n",
        "        f\"c=0,1,2,3,4,5,6,64,67,65,66\"\n",
        "    ),\n",
        "    \"Growth\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=an_recom_buybetter,fa_epsqoq_o20,fa_salesqoq_o20,\"\n",
        "        f\"ind_stocksonly,sh_avgvol_o1000,sh_price_o10,ta_perf_4wup,\"\n",
        "        f\"ta_perf2_13wup,ta_sma20_pa,ta_sma200_pa,ta_sma50_pa&ft=4\"\n",
        "    ),\n",
        "    \"IPO\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=ind_stocksonly,ipodate_prev2yrs,sh_avgvol_o1000,sh_price_o10,\"\n",
        "        f\"ta_sma20_pa&ft=4\"\n",
        "    ),\n",
        "    \"52 Week High\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=sh_avgvol_o1000,sh_price_o10,ta_beta_o1,ta_highlow52w_nh&ft=4\"\n",
        "    ),\n",
        "    \"Week 20%+ Gain\": (\n",
        "        f\"{FINVIZ_BASE}/screener.ashx?v=111\"\n",
        "        f\"&f=cap_smallover,sh_avgvol_o1000,sh_price_o3,ta_perf_1w20o,\"\n",
        "        f\"ta_volatility_wo4&ft=4&o=-marketcap&r=25\"\n",
        "    )\n",
        "}\n",
        "\n",
        "def fetch_all_tickers(screener_url: str, max_pages: int = 10) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch tickers from a Finviz screener URL, paginating up to max_pages.\n",
        "    Returns DataFrame with columns: No., Ticker, Company, Sector, Industry,\n",
        "    Country, Market Cap, P/E, Volume, Price, Change.\n",
        "    \"\"\"\n",
        "    combined = []\n",
        "    seen = set()\n",
        "    page = 1\n",
        "\n",
        "    while page <= max_pages:\n",
        "        resp = session.get(f\"{screener_url}&r={1+(page-1)*20}\", timeout=10)\n",
        "        if resp.status_code != 200:\n",
        "            break\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "        rows = soup.select('tr[valign=\"top\"]')\n",
        "        if not rows:\n",
        "            break\n",
        "\n",
        "        new_data = False\n",
        "        for row in rows:\n",
        "            cols = row.find_all('td')\n",
        "            if len(cols) == 11:\n",
        "                ticker = cols[1].text.strip()\n",
        "                if ticker and ticker not in seen:\n",
        "                    combined.append([c.text.strip() for c in cols])\n",
        "                    seen.add(ticker)\n",
        "                    new_data = True\n",
        "        if not new_data:\n",
        "            break\n",
        "        page += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "    columns = ['No.', 'Ticker', 'Company', 'Sector', 'Industry',\n",
        "               'Country', 'Market Cap', 'P/E', 'Volume', 'Price', 'Change']\n",
        "    return pd.DataFrame(combined, columns=columns) if combined else pd.DataFrame(columns=columns)\n",
        "\n",
        "\n",
        "def aggregate_and_save(screener_map: dict) -> (pd.DataFrame, str, str):\n",
        "    \"\"\"\n",
        "    Aggregates tickers across screeners, saves CSV & HTML summary.\n",
        "    Returns (DataFrame, csv_path, html_path).\n",
        "    \"\"\"\n",
        "    mapping = defaultdict(list)\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    for name, url in screener_map.items():\n",
        "        df = fetch_all_tickers(url)\n",
        "        print(f\"{name}: {len(df)} tickers found\")\n",
        "        for t in df['Ticker'].unique():\n",
        "            mapping[t].append(name)\n",
        "\n",
        "    if not mapping:\n",
        "        mapping['TSLA'].append('Default Screener')\n",
        "\n",
        "    data = []\n",
        "    for t, screens in mapping.items():\n",
        "        data.append({\n",
        "            'Ticker': t,\n",
        "            'Appearances': len(screens),\n",
        "            'Screeners': \", \".join(screens)\n",
        "        })\n",
        "    summary_df = pd.DataFrame(data).sort_values(['Appearances','Ticker'], ascending=[False, True])\n",
        "\n",
        "    csv_file = f\"finviz_screeners_{today}.csv\"\n",
        "    html_file = f\"finviz_screeners_{today}.html\"\n",
        "    summary_df.to_csv(csv_file, index=False)\n",
        "    summary_df.to_html(html_file, index=False)\n",
        "\n",
        "    return summary_df, csv_file, html_file\n",
        "\n",
        "# ----------------------------\n",
        "# Part 2: ATR% Scraper & Filter\n",
        "# ----------------------------\n",
        "\n",
        "def get_atr_pct(ticker: str) -> float:\n",
        "    \"\"\"\n",
        "    Scrapes Finviz for ATR (14) and Prev Close, returns ATR% or None.\n",
        "    \"\"\"\n",
        "    resp = session.get(f\"{FINVIZ_BASE}/quote.ashx\", params={\"t\": ticker})\n",
        "    resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "    table = soup.find(\"table\", class_=\"snapshot-table2\")\n",
        "    if not table:\n",
        "        return None\n",
        "    data = {}\n",
        "    for row in table.find_all(\"tr\"):\n",
        "        cells = row.find_all(\"td\")\n",
        "        for key_cell, val_cell in zip(cells[0::2], cells[1::2]):\n",
        "            data[key_cell.get_text(strip=True)] = val_cell.get_text(strip=True)\n",
        "    atr = data.get(\"ATR\")\n",
        "    prev = data.get(\"Prev Close\")\n",
        "    try:\n",
        "        return (float(atr) / float(prev)) * 100\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# ----------------------------\n",
        "# Part 3: Chart Gallery\n",
        "# ----------------------------\n",
        "\n",
        "def generate_finviz_gallery(tickers: list) -> str:\n",
        "    \"\"\"\n",
        "    Creates HTML displaying a 4-col grid of Finviz charts, dated filename.\n",
        "    \"\"\"\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    out_html = f\"finviz_chart_grid_{today}.html\"\n",
        "    html = [\n",
        "        '<html><head><title>Finviz Chart Gallery</title>',\n",
        "        '<style>',\n",
        "        'body { font-family: Arial; background: #f5f5f5; padding: 20px }',\n",
        "        '.chart-grid { display: flex; flex-wrap: wrap; gap: 20px }',\n",
        "        '.chart-item { width: 23%; background: white; border:1px solid #ccc;',\n",
        "        'padding:10px; box-shadow:2px 2px 5px rgba(0,0,0,0.1); text-align:center }',\n",
        "        '.chart-item img { max-width:100%; height:auto }',\n",
        "        'h2 { text-align:center }',\n",
        "        '</style></head><body><h2>Finviz Chart Gallery</h2>',\n",
        "        '<div class=\"chart-grid\">'\n",
        "    ]\n",
        "    for t in tickers:\n",
        "        params = {'t': t, 'ty': 'c', 'ta': '1', 'p': 'd', 's': 'm'}\n",
        "        img_src = requests.Request('GET', f\"{FINVIZ_BASE}/chart.ashx\", params=params).prepare().url\n",
        "        html.append(f'<div class=\"chart-item\"><h4>{t}</h4>'\n",
        "                    f'<img src=\"{img_src}\" alt=\"{t}\"></div>')\n",
        "    html.append('</div></body></html>')\n",
        "    with open(out_html, 'w') as f:\n",
        "        f.write(\"\\n\".join(html))\n",
        "    return out_html\n",
        "\n",
        "# ----------------------------\n",
        "# Main Execution\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Aggregate screeners\n",
        "    summary_df, csv_path, html_summary = aggregate_and_save(screener_urls)\n",
        "    print(f\"Total tickers before ATR% filter: {len(summary_df)}\")\n",
        "\n",
        "    # 2. Compute ATR% and filter >3%\n",
        "    summary_df['ATR%'] = summary_df['Ticker'].apply(get_atr_pct)\n",
        "    filtered_df = summary_df[summary_df['ATR%'] > 3.0]\n",
        "    print(f\"Tickers with ATR% > 3%: {len(filtered_df)}\")\n",
        "\n",
        "    # 3. Save filtered results\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    filtered_csv = f\"finviz_screeners_filtered_{today}.csv\"\n",
        "    filtered_html = f\"finviz_screeners_filtered_{today}.html\"\n",
        "    filtered_df.to_csv(filtered_csv, index=False)\n",
        "    filtered_df.to_html(filtered_html, index=False)\n",
        "    print(f\"Filtered CSV: {filtered_csv}\\nFiltered HTML: {filtered_html}\")\n",
        "\n",
        "    # 4. Generate gallery for filtered tickers\n",
        "    gallery_path = generate_finviz_gallery(filtered_df['Ticker'].tolist())\n",
        "    print(f\"Filtered chart gallery HTML: {gallery_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "NyxpzJwP3KIH",
        "outputId": "a2c79779-3969-49a8-f7a3-482295201d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10% Change: 13 tickers found\n",
            "Growth: 34 tickers found\n",
            "IPO: 46 tickers found\n",
            "52 Week High: 11 tickers found\n",
            "Week 20%+ Gain: 37 tickers found\n",
            "Total tickers before ATR% filter: 125\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "429 Client Error: Too Many Requests for url: https://finviz.com/quote.ashx?t=ARM",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mget_atr_pct\u001b[0;34m(ticker)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://finviz.com/quote.ashx?t=ARM"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E64jAhXm3RsV",
        "outputId": "269ea0b6-4436-409b-8d04-8bbf603e7217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.50)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (5.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.3.6)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2024.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.8)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2024.8.30)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=ee45fdaf7ab428675bdc990939e8c4107b01207f370b01788f780a0a7c8f0598\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/67/4f/8a9f252836e053e532c6587a3230bc72a4deb16b03a829610b\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance ta pandas beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tSRH0lyN3V2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from data_fetcher import fetch_data_from_url_with_pagination\n",
        "from calculations import calculate_atr, calculate_atr_percentage, add_50_ma, calculate_percent_gain_from_50ma\n",
        "from filters import filter_by_ma_atr_ratio\n",
        "\n"
      ],
      "metadata": {
        "id": "nOe8shCU4d_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zH5UR7Ho4fa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define screener URLs\n",
        "urls = {\n",
        "#    \"Screener1\": \"https://finviz.com/screener.ashx?v=151&f=sh_avgvol_o1000,sh_price_o10,ta_beta_o1,ta_highlow52w_nh&ft=4\"\n",
        "    \"Screener2\": \"https://finviz.com/screener.ashx?v=151&f=ind_stocksonly,sh_avgvol_o1000,sh_price_o10,ta_change_u10,ta_sma20_sa50,ta_sma50_pa&ft=4\"\n",
        "#    \"Screener3\": \"https://finviz.com/screener.ashx?v=151&f=cap_smallover,sh_avgvol_o1000,sh_curvol_o100,ta_perf_13w50o,ta_volatility_mo5&ft=4&o=-marketcap&ar=180\"\n",
        "}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Store filtered results for each screener\n",
        "    screener_results_atr = {}\n",
        "    screener_results_ma_atr_ratio = {}\n",
        "\n",
        "    for screener_name, url in urls.items():\n",
        "        print(f\"Fetching data from {screener_name}...\")\n",
        "        df = fetch_data_from_url_with_pagination(url)\n",
        "\n",
        "        # Fetch tickers\n",
        "        tickers = df['Ticker'].tolist()\n",
        "\n",
        "        # Calculate ATR for each ticker\n",
        "        # print(f\"Calculating ATR for tickers in {screener_name}...\")\n",
        "        df['ATR'] = [calculate_atr(ticker, period='1mo', atr_window=14) for ticker in df['Ticker']]\n",
        "\n",
        "        # Debug ATR column\n",
        "        print(f\"ATR values for {screener_name}:\")\n",
        "        print(df[['Ticker', 'ATR']])\n",
        "\n",
        "        # Calculate ATR% for each ticker\n",
        "        print(f\"Calculating ATR% for tickers in {screener_name}...\")\n",
        "        df = calculate_atr_percentage(df)\n",
        "\n",
        "        # Debug ATR% values\n",
        "        print(f\"ATR% values for {screener_name}:\")\n",
        "        print(df[['Ticker', 'ATR%', 'Price']])\n",
        "\n",
        "        # # Add 50 MA to the DataFrame\n",
        "        # print(f\"Calculating 50 MA for {screener_name}...\")\n",
        "        # tickers = df['Ticker'].tolist()\n",
        "        # df = add_50_ma(df, tickers)\n",
        "\n",
        "        # # Calculate % Gain from 50 MA\n",
        "        # print(f\"Calculating % Gain from 50 MA for {screener_name}...\")\n",
        "        # df = calculate_percent_gain_from_50ma(df)\n",
        "\n",
        "        # # Filter based on MA-ATR Ratio\n",
        "        # print(f\"Applying MA-ATR Ratio filter for {screener_name}...\")\n",
        "        # ma_atr_filtered_df = filter_by_ma_atr_ratio(df, threshold=1)\n",
        "\n",
        "        # # Include relevant columns for output\n",
        "        # ma_atr_filtered_df = ma_atr_filtered_df[\n",
        "        #     ['Ticker', 'Price', 'ATR', 'ATR%', '% Gain from 50 MA', 'MA_ATR_Ratio']\n",
        "        # ]\n",
        "\n",
        "        # screener_results_ma_atr_ratio[screener_name] = ma_atr_filtered_df\n",
        "\n",
        "        # print(f\"MA-ATR Ratio Filtered results for {screener_name}:\")\n",
        "        # print(ma_atr_filtered_df)\n",
        "\n",
        "    # Save filtered results to CSV\n",
        "    for screener_name, atr_filtered_df in screener_results_atr.items():\n",
        "        atr_filtered_df.to_csv(f\"{screener_name}_atr_filtered.csv\", index=False)\n",
        "\n",
        "    # for screener_name, ma_atr_filtered_df in screener_results_ma_atr_ratio.items():\n",
        "    #     ma_atr_filtered_df.to_csv(f\"{screener_name}_ma_atr_filtered.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiZli1Ph4odF",
        "outputId": "c93247a2-14cb-4083-cb37-ebf66dc834c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data from Screener2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected multi-index columns for AAOI. Normalizing...\n",
            "Detected multi-index columns for CRDO. Normalizing...\n",
            "Detected multi-index columns for IONQ. Normalizing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected multi-index columns for MRVL. Normalizing...\n",
            "Detected multi-index columns for NVCR. Normalizing...\n",
            "Detected multi-index columns for SERV. Normalizing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected multi-index columns for SOUN. Normalizing...\n",
            "Detected multi-index columns for UMAC. Normalizing...\n",
            "ATR values for Screener2:\n",
            "  Ticker       ATR\n",
            "0   AAOI  4.272582\n",
            "1   CRDO  5.278336\n",
            "2   IONQ  4.216305\n",
            "3   MRVL  5.325973\n",
            "4   NVCR  2.246982\n",
            "5   SERV  1.352008\n",
            "6   SOUN  1.475243\n",
            "7   UMAC  2.461471\n",
            "Calculating ATR% for tickers in Screener2...\n",
            "0    4.272582\n",
            "1    5.278336\n",
            "2    4.216305\n",
            "3    5.325973\n",
            "4    2.246982\n",
            "5    1.352008\n",
            "6    1.475243\n",
            "7    2.461471\n",
            "Name: ATR, dtype: float64\n",
            "0     36.43\n",
            "1     75.95\n",
            "2     33.83\n",
            "3    120.77\n",
            "4     33.41\n",
            "5     13.08\n",
            "6     16.91\n",
            "7     11.10\n",
            "Name: Price, dtype: float64\n",
            "ATR% values for Screener2:\n",
            "  Ticker       ATR%   Price\n",
            "0   AAOI  11.728195   36.43\n",
            "1   CRDO   6.949751   75.95\n",
            "2   IONQ  12.463213   33.83\n",
            "3   MRVL   4.410014  120.77\n",
            "4   NVCR   6.725476   33.41\n",
            "5   SERV  10.336453   13.08\n",
            "6   SOUN   8.724088   16.91\n",
            "7   UMAC  22.175419   11.10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import calculations  # Import your file\n",
        "\n",
        "# Reload the file if changes are made\n",
        "importlib.reload(calculations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nmju7tM4vrL",
        "outputId": "03695ec3-09ed-4cc5-a57b-d51ac4fd26c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'calculations' from '/content/calculations.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tuTdYS2a8362",
        "outputId": "02e83119-c948-4504-c6ca-95272352aaa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1csdhvYl9AYe",
        "outputId": "d573f0d2-1adf-4aa5-c1d5-3ab3e2be2c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'data_fetcher.py',\n",
              " '__pycache__',\n",
              " 'utils.py',\n",
              " 'filters.py',\n",
              " 'requirements.txt',\n",
              " 'main.py',\n",
              " 'calculations.py',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define screener URLs\n",
        "urls = {\n",
        "#    \"Screener1\": \"https://finviz.com/screener.ashx?v=151&f=sh_avgvol_o1000,sh_price_o10,ta_beta_o1,ta_highlow52w_nh&ft=4\"\n",
        "    \"Screener2\": \"https://finviz.com/screener.ashx?v=151&f=ind_stocksonly,sh_avgvol_o1000,sh_price_o10,ta_change_u10,ta_sma20_sa50,ta_sma50_pa&ft=4\"\n",
        "#    \"Screener3\": \"https://finviz.com/screener.ashx?v=151&f=cap_smallover,sh_avgvol_o1000,sh_curvol_o100,ta_perf_13w50o,ta_volatility_mo5&ft=4&o=-marketcap&ar=180\"\n",
        "}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Store filtered results for each screener\n",
        "    screener_results_atr = {}\n",
        "    screener_results_ma_atr_ratio = {}\n",
        "\n",
        "    for screener_name, url in urls.items():\n",
        "        print(f\"Fetching data from {screener_name}...\")\n",
        "        df = fetch_data_from_url_with_pagination(url)\n",
        "\n",
        "        # Fetch tickers\n",
        "        tickers = df['Ticker'].tolist()\n",
        "\n",
        "        # Calculate ATR for each ticker\n",
        "        # print(f\"Calculating ATR for tickers in {screener_name}...\")\n",
        "        df['ATR'] = [calculate_atr(ticker, period='1mo', atr_window=14) for ticker in df['Ticker']]\n",
        "\n",
        "        # Debug ATR column\n",
        "        print(f\"ATR values for {screener_name}:\")\n",
        "        print(df[['Ticker', 'ATR']])\n",
        "\n",
        "        # Calculate ATR% for each ticker\n",
        "        print(f\"Calculating ATR% for tickers in {screener_name}...\")\n",
        "        df = calculate_atr_percentage(df)\n",
        "\n",
        "        # Debug ATR% values\n",
        "        print(f\"ATR% values for {screener_name}:\")\n",
        "        print(df[['Ticker', 'ATR', 'Price', 'ATR%']])\n",
        "\n",
        "        # # Add 50 MA to the DataFrame\n",
        "        # print(f\"Calculating 50 MA for {screener_name}...\")\n",
        "        # tickers = df['Ticker'].tolist()\n",
        "        # df = add_50_ma(df, tickers)\n",
        "\n",
        "        # # Calculate % Gain from 50 MA\n",
        "        # print(f\"Calculating % Gain from 50 MA for {screener_name}...\")\n",
        "        # df = calculate_percent_gain_from_50ma(df)\n",
        "\n",
        "        # # Filter based on MA-ATR Ratio\n",
        "        # print(f\"Applying MA-ATR Ratio filter for {screener_name}...\")\n",
        "        # ma_atr_filtered_df = filter_by_ma_atr_ratio(df, threshold=1)\n",
        "\n",
        "        # # Include relevant columns for output\n",
        "        # ma_atr_filtered_df = ma_atr_filtered_df[\n",
        "        #     ['Ticker', 'Price', 'ATR', 'ATR%', '% Gain from 50 MA', 'MA_ATR_Ratio']\n",
        "        # ]\n",
        "\n",
        "        # screener_results_ma_atr_ratio[screener_name] = ma_atr_filtered_df\n",
        "\n",
        "        # print(f\"MA-ATR Ratio Filtered results for {screener_name}:\")\n",
        "        # print(ma_atr_filtered_df)\n",
        "\n",
        "    # Save filtered results to CSV\n",
        "    for screener_name, atr_filtered_df in screener_results_atr.items():\n",
        "        atr_filtered_df.to_csv(f\"{screener_name}_atr_filtered.csv\", index=False)\n",
        "\n",
        "    # for screener_name, ma_atr_filtered_df in screener_results_ma_atr_ratio.items():\n",
        "    #     ma_atr_filtered_df.to_csv(f\"{screener_name}_ma_atr_filtered.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-s7f3iP9Cqd",
        "outputId": "18700731-b257-4a05-9fa2-810775c939a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data from Screener2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected multi-index columns for AAOI. Normalizing...\n",
            "Detected multi-index columns for CRDO. Normalizing...\n",
            "Detected multi-index columns for IONQ. Normalizing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected multi-index columns for MRVL. Normalizing...\n",
            "Detected multi-index columns for NVCR. Normalizing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected multi-index columns for SERV. Normalizing...\n",
            "Detected multi-index columns for SOUN. Normalizing...\n",
            "Detected multi-index columns for UMAC. Normalizing...\n",
            "ATR values for Screener2:\n",
            "  Ticker       ATR\n",
            "0   AAOI  4.272582\n",
            "1   CRDO  5.278336\n",
            "2   IONQ  4.216305\n",
            "3   MRVL  5.325973\n",
            "4   NVCR  2.246982\n",
            "5   SERV  1.352008\n",
            "6   SOUN  1.475243\n",
            "7   UMAC  2.461471\n",
            "Calculating ATR% for tickers in Screener2...\n",
            "ATR% values for Screener2:\n",
            "  Ticker       ATR   Price       ATR%\n",
            "0   AAOI  4.272582   36.43  11.728195\n",
            "1   CRDO  5.278336   75.95   6.949751\n",
            "2   IONQ  4.216305   33.83  12.463213\n",
            "3   MRVL  5.325973  120.77   4.410014\n",
            "4   NVCR  2.246982   33.41   6.725476\n",
            "5   SERV  1.352008   13.08  10.336453\n",
            "6   SOUN  1.475243   16.91   8.724088\n",
            "7   UMAC  2.461471   11.10  22.175419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import calculations  # The file where you made changes\n",
        "\n",
        "# Reload the module to reflect changes\n",
        "importlib.reload(calculations)\n",
        "\n",
        "# Import functions after reloading\n",
        "from calculations import calculate_atr, calculate_atr_percentage\n"
      ],
      "metadata": {
        "id": "T_4S64ry9LbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H = 38.24\n",
        "L = 33.56\n",
        "C = 36.43\n",
        "\n",
        "ATR = max(abs(H - L), abs(H - C), abs(L - C))\n",
        "\n",
        "print(ATR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltwYLhaN9qOE",
        "outputId": "3d5288e7-c0d8-4ee6-fd5d-13e23d32302e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time"
      ],
      "metadata": {
        "id": "rHyIUg-7JYRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Screener URLs\n",
        "screener_urls = {\n",
        "    \"10% Change\": \"https://finviz.com/screener.ashx?v=151&f=ind_stocksonly,sh_avgvol_o500,sh_price_o5,ta_changeopen_u10,ta_sma20_sa50,ta_sma50_pa&ft=4&o=-relativevolume&c=0,1,2,3,4,5,6,64,67,65,66\",\n",
        "    \"Growth\": \"https://finviz.com/screener.ashx?v=111&f=an_recom_buybetter,fa_epsqoq_o20,fa_salesqoq_o20,ind_stocksonly,sh_avgvol_o1000,sh_price_o10,ta_perf_4wup,ta_perf2_13wup,ta_sma20_pa,ta_sma200_pa,ta_sma50_pa&ft=4\",\n",
        "    \"IPO\": \"https://finviz.com/screener.ashx?v=211&f=ind_stocksonly,ipodate_prev2yrs,sh_avgvol_o1000,sh_price_o10,ta_sma20_pa&ft=4\",\n",
        "    \"52 Week High\": \"https://finviz.com/screener.ashx?v=211&f=sh_avgvol_o1000,sh_price_o10,ta_beta_o1,ta_highlow52w_nh&ft=4\",\n",
        "    \"Week 20%+ Gain\": \"https://finviz.com/screener.ashx?v=211&f=cap_smallover,sh_avgvol_o1000,sh_price_o3,ta_perf_1w20o,ta_volatility_wo4&ft=4&o=-marketcap&r=25\"\n",
        "}\n",
        "\n",
        "def fetch_all_tickers_from_screener(screener_url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                      \"Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    session = requests.Session()\n",
        "    session.headers.update(headers)\n",
        "\n",
        "    combined_data = []\n",
        "    page = 1\n",
        "\n",
        "    while True:\n",
        "        paged_url = f\"{screener_url}&r={1 + (page - 1) * 20}\"\n",
        "        response = session.get(paged_url)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        rows = soup.select('tr[valign=\"top\"]')\n",
        "\n",
        "        if not rows:\n",
        "            break\n",
        "\n",
        "        for row in rows:\n",
        "            cells = row.find_all('td')\n",
        "            if len(cells) == 11:\n",
        "                combined_data.append([cell.text.strip() for cell in cells])\n",
        "\n",
        "        page += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "    columns = [\n",
        "        'No.', 'Ticker', 'Company', 'Sector', 'Industry',\n",
        "        'Country', 'Market Cap', 'P/E', 'Volume', 'Price', 'Change'\n",
        "    ]\n",
        "\n",
        "    if combined_data:\n",
        "        df = pd.DataFrame(combined_data, columns=columns)\n",
        "    else:\n",
        "        df = pd.DataFrame(columns=columns)\n",
        "\n",
        "    return df\n",
        "\n",
        "def aggregate_tickers(screener_urls):\n",
        "    ticker_to_screeners = defaultdict(list)\n",
        "\n",
        "    for name, url in screener_urls.items():\n",
        "        df = fetch_all_tickers_from_screener(url)\n",
        "        tickers = df['Ticker'].tolist()\n",
        "\n",
        "        print(f\"{name} fetched {len(tickers)} tickers.\")\n",
        "\n",
        "        for ticker in tickers:\n",
        "            ticker_to_screeners[ticker].append(name)\n",
        "\n",
        "    if not ticker_to_screeners:\n",
        "        print(\"All screeners returned no results. Adding default ticker TSLA.\")\n",
        "        ticker_to_screeners['TSLA'].append('Default Screener')\n",
        "\n",
        "    return ticker_to_screeners\n",
        "\n",
        "def save_results(ticker_to_screeners):\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    all_data = []\n",
        "\n",
        "    for ticker, screeners in ticker_to_screeners.items():\n",
        "        all_data.append({\n",
        "            \"Ticker\": ticker,\n",
        "            \"Appearances\": len(screeners),\n",
        "            \"Screeners\": \", \".join(screeners)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.sort_values(by=[\"Appearances\", \"Ticker\"], ascending=[False, True], inplace=True)\n",
        "\n",
        "    filename = f\"finviz_screeners_{today}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"\\nResults saved to {filename}\")\n",
        "\n",
        "def display_results(ticker_to_screeners):\n",
        "    print(\"\\n=== All Tickers Grouped by Screener ===\\n\")\n",
        "    for ticker, screeners in ticker_to_screeners.items():\n",
        "        print(f\"{ticker}: from {', '.join(screeners)}\")\n",
        "\n",
        "    print(\"\\n=== Strong Overlap Candidates (Appearing in Multiple Screeners) ===\\n\")\n",
        "    for ticker, screeners in ticker_to_screeners.items():\n",
        "        if len(screeners) > 1:\n",
        "            print(f\"{ticker}: Appears in {len(screeners)} screeners -> {', '.join(screeners)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ticker_to_screeners = aggregate_tickers(screener_urls)\n",
        "    display_results(ticker_to_screeners)\n",
        "    save_results(ticker_to_screeners)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "x6C8JA0TK22t",
        "outputId": "fb391394-cc40-403d-f653-6f5080cc065d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e413ce0b6ef0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mticker_to_screeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregate_tickers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreener_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0mdisplay_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker_to_screeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0msave_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker_to_screeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e413ce0b6ef0>\u001b[0m in \u001b[0;36maggregate_tickers\u001b[0;34m(screener_urls)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscreener_urls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_all_tickers_from_screener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mtickers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ticker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e413ce0b6ef0>\u001b[0m in \u001b[0;36mfetch_all_tickers_from_screener\u001b[0;34m(screener_url)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mpage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     columns = [\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Screener URLs\n",
        "screener_urls = {\n",
        "    \"10% Change\": \"https://finviz.com/screener.ashx?v=151&f=ind_stocksonly,sh_avgvol_o500,sh_price_o5,ta_changeopen_u10,ta_sma20_sa50,ta_sma50_pa&ft=4&o=-relativevolume&c=0,1,2,3,4,5,6,64,67,65,66\",\n",
        "    \"Growth\": \"https://finviz.com/screener.ashx?v=111&f=an_recom_buybetter,fa_epsqoq_o20,fa_salesqoq_o20,ind_stocksonly,sh_avgvol_o1000,sh_price_o10,ta_perf_4wup,ta_perf2_13wup,ta_sma20_pa,ta_sma200_pa,ta_sma50_pa&ft=4\",\n",
        "    \"IPO\": \"https://finviz.com/screener.ashx?v=211&f=ind_stocksonly,ipodate_prev2yrs,sh_avgvol_o1000,sh_price_o10,ta_sma20_pa&ft=4\",\n",
        "    \"52 Week High\": \"https://finviz.com/screener.ashx?v=211&f=sh_avgvol_o1000,sh_price_o10,ta_beta_o1,ta_highlow52w_nh&ft=4\",\n",
        "    \"Week 20%+ Gain\": \"https://finviz.com/screener.ashx?v=211&f=cap_smallover,sh_avgvol_o1000,sh_price_o3,ta_perf_1w20o,ta_volatility_wo4&ft=4&o=-marketcap&r=25\"\n",
        "}\n",
        "\n",
        "def fetch_all_tickers_from_screener(screener_url, max_pages=10):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                      \"Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    session = requests.Session()\n",
        "    session.headers.update(headers)\n",
        "\n",
        "    combined_data = []\n",
        "    page = 1\n",
        "\n",
        "    while page <= max_pages:\n",
        "        paged_url = f\"{screener_url}&r={1 + (page - 1) * 20}\"\n",
        "        try:\n",
        "            response = session.get(paged_url, timeout=10)\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"Timeout fetching page {page}. Skipping...\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching page {page}: {e}\")\n",
        "            break\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        rows = soup.select('tr[valign=\"top\"]')\n",
        "\n",
        "        if not rows:\n",
        "            print(f\"No rows found on page {page}. Ending pagination.\")\n",
        "            break\n",
        "\n",
        "        page_has_valid_data = False\n",
        "        for row in rows:\n",
        "            cells = row.find_all('td')\n",
        "            if len(cells) == 11:\n",
        "                combined_data.append([cell.text.strip() for cell in cells])\n",
        "                page_has_valid_data = True\n",
        "\n",
        "        if not page_has_valid_data:\n",
        "            print(f\"No valid stock entries found on page {page}. Ending pagination.\")\n",
        "            break\n",
        "\n",
        "        page += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "    columns = [\n",
        "        'No.', 'Ticker', 'Company', 'Sector', 'Industry',\n",
        "        'Country', 'Market Cap', 'P/E', 'Volume', 'Price', 'Change'\n",
        "    ]\n",
        "\n",
        "    if combined_data:\n",
        "        df = pd.DataFrame(combined_data, columns=columns)\n",
        "    else:\n",
        "        df = pd.DataFrame(columns=columns)\n",
        "\n",
        "    return df\n",
        "\n",
        "def aggregate_tickers(screener_urls):\n",
        "    ticker_to_screeners = defaultdict(list)\n",
        "\n",
        "    for name, url in screener_urls.items():\n",
        "        df = fetch_all_tickers_from_screener(url)\n",
        "        tickers = df['Ticker'].tolist()\n",
        "\n",
        "        print(f\"{name} fetched {len(tickers)} tickers.\")\n",
        "\n",
        "        for ticker in tickers:\n",
        "            ticker_to_screeners[ticker].append(name)\n",
        "\n",
        "    if not ticker_to_screeners:\n",
        "        print(\"All screeners returned no results. Adding default ticker TSLA.\")\n",
        "        ticker_to_screeners['TSLA'].append('Default Screener')\n",
        "\n",
        "    return ticker_to_screeners\n",
        "\n",
        "def save_results(ticker_to_screeners):\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    all_data = []\n",
        "\n",
        "    for ticker, screeners in ticker_to_screeners.items():\n",
        "        all_data.append({\n",
        "            \"Ticker\": ticker,\n",
        "            \"Appearances\": len(screeners),\n",
        "            \"Screeners\": \", \".join(screeners)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.sort_values(by=[\"Appearances\", \"Ticker\"], ascending=[False, True], inplace=True)\n",
        "\n",
        "    filename = f\"finviz_screeners_{today}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"\\nResults saved to {filename}\")\n",
        "\n",
        "def display_results(ticker_to_screeners):\n",
        "    print(\"\\n=== All Tickers Grouped by Screener ===\\n\")\n",
        "    for ticker, screeners in ticker_to_screeners.items():\n",
        "        print(f\"{ticker}: from {', '.join(screeners)}\")\n",
        "\n",
        "    print(\"\\n=== Strong Overlap Candidates (Appearing in Multiple Screeners) ===\\n\")\n",
        "    for ticker, screeners in ticker_to_screeners.items():\n",
        "        if len(screeners) > 1:\n",
        "            print(f\"{ticker}: Appears in {len(screeners)} screeners -> {', '.join(screeners)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ticker_to_screeners = aggregate_tickers(screener_urls)\n",
        "    display_results(ticker_to_screeners)\n",
        "    save_results(ticker_to_screeners)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A_xQZnWNtlj",
        "outputId": "d97102a1-f437-432f-a725-9b810a122ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10% Change fetched 11 tickers.\n",
            "Growth fetched 27 tickers.\n",
            "No rows found on page 1. Ending pagination.\n",
            "IPO fetched 0 tickers.\n",
            "No rows found on page 1. Ending pagination.\n",
            "52 Week High fetched 0 tickers.\n",
            "No rows found on page 1. Ending pagination.\n",
            "Week 20%+ Gain fetched 0 tickers.\n",
            "\n",
            "=== All Tickers Grouped by Screener ===\n",
            "\n",
            "UPXI: from 10% Change\n",
            "EPWK: from 10% Change, 10% Change, 10% Change, 10% Change, 10% Change, 10% Change, 10% Change, 10% Change, 10% Change, 10% Change\n",
            "ADMA: from Growth\n",
            "AEM: from Growth\n",
            "AGI: from Growth\n",
            "ATEC: from Growth\n",
            "CI: from Growth\n",
            "CNK: from Growth\n",
            "EAT: from Growth\n",
            "GENI: from Growth\n",
            "GH: from Growth\n",
            "GRAL: from Growth\n",
            "KGC: from Growth\n",
            "LLY: from Growth\n",
            "NEM: from Growth\n",
            "TGTX: from Growth\n",
            "UBER: from Growth\n",
            "WGS: from Growth\n",
            "WTRG: from Growth\n",
            "ZS: from Growth, Growth, Growth, Growth, Growth, Growth, Growth, Growth, Growth, Growth\n",
            "\n",
            "=== Strong Overlap Candidates (Appearing in Multiple Screeners) ===\n",
            "\n",
            "EPWK: Appears in 10 screeners -> 10% Change, 10% Change, 10% Change, 10% Change, 10% Change, 10% Change, 10% Change, 10% Change, 10% Change, 10% Change\n",
            "ZS: Appears in 10 screeners -> Growth, Growth, Growth, Growth, Growth, Growth, Growth, Growth, Growth, Growth\n",
            "\n",
            "Results saved to finviz_screeners_2025-04-25.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "# Screener URLs\n",
        "screener_urls = {\n",
        "    \"10% Change\": \"https://finviz.com/screener.ashx?v=151&f=ind_stocksonly,sh_avgvol_o500,sh_price_o5,ta_changeopen_u10,ta_sma20_sa50,ta_sma50_pa&ft=4&o=-relativevolume&c=0,1,2,3,4,5,6,64,67,65,66\",\n",
        "    \"Growth\": \"https://finviz.com/screener.ashx?v=111&f=an_recom_buybetter,fa_epsqoq_o20,fa_salesqoq_o20,ind_stocksonly,sh_avgvol_o1000,sh_price_o10,ta_perf_4wup,ta_perf2_13wup,ta_sma20_pa,ta_sma200_pa,ta_sma50_pa&ft=4\",\n",
        "    \"IPO\": \"https://finviz.com/screener.ashx?v=111&f=ind_stocksonly,ipodate_prev2yrs,sh_avgvol_o1000,sh_price_o10,ta_sma20_pa&ft=4\",\n",
        "    \"52 Week High\": \"https://finviz.com/screener.ashx?v=111&f=sh_avgvol_o1000,sh_price_o10,ta_beta_o1,ta_highlow52w_nh&ft=4\",\n",
        "    \"Week 20%+ Gain\": \"https://finviz.com/screener.ashx?v=111&f=cap_smallover,sh_avgvol_o1000,sh_price_o3,ta_perf_1w20o,ta_volatility_wo4&ft=4&o=-marketcap&r=25\"\n",
        "}\n",
        "\n",
        "def fetch_all_tickers_from_screener(screener_url, max_pages=10):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                      \"Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    session = requests.Session()\n",
        "    session.headers.update(headers)\n",
        "\n",
        "    combined_data = []\n",
        "    seen_tickers = set()\n",
        "    page = 1\n",
        "\n",
        "    while page <= max_pages:\n",
        "        paged_url = f\"{screener_url}&r={1 + (page - 1) * 20}\"\n",
        "        try:\n",
        "            response = session.get(paged_url, timeout=10)\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"Timeout fetching page {page}. Skipping...\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching page {page}: {e}\")\n",
        "            break\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        rows = soup.select('tr[valign=\"top\"]')\n",
        "\n",
        "        if not rows:\n",
        "            print(f\"No rows found on page {page}. Ending pagination.\")\n",
        "            break\n",
        "\n",
        "        page_new_data = False\n",
        "        for row in rows:\n",
        "            cells = row.find_all('td')\n",
        "            if len(cells) == 11:\n",
        "                ticker = cells[1].text.strip()\n",
        "                if ticker and ticker not in seen_tickers:\n",
        "                    combined_data.append([cell.text.strip() for cell in cells])\n",
        "                    seen_tickers.add(ticker)\n",
        "                    page_new_data = True\n",
        "\n",
        "        if not page_new_data:\n",
        "            print(f\"No new tickers found on page {page}. Ending pagination.\")\n",
        "            break\n",
        "\n",
        "        page += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "    columns = [\n",
        "        'No.', 'Ticker', 'Company', 'Sector', 'Industry',\n",
        "        'Country', 'Market Cap', 'P/E', 'Volume', 'Price', 'Change'\n",
        "    ]\n",
        "\n",
        "    if combined_data:\n",
        "        df = pd.DataFrame(combined_data, columns=columns)\n",
        "    else:\n",
        "        df = pd.DataFrame(columns=columns)\n",
        "\n",
        "    return df\n",
        "\n",
        "def aggregate_tickers(screener_urls):\n",
        "    ticker_to_screeners = defaultdict(list)\n",
        "\n",
        "    for name, url in screener_urls.items():\n",
        "        df = fetch_all_tickers_from_screener(url)\n",
        "        tickers = df['Ticker'].tolist()\n",
        "\n",
        "        print(f\"{name} fetched {len(tickers)} tickers.\")\n",
        "\n",
        "        for ticker in set(tickers):\n",
        "            ticker_to_screeners[ticker].append(name)\n",
        "\n",
        "    if not ticker_to_screeners:\n",
        "        print(\"All screeners returned no results. Adding default ticker TSLA.\")\n",
        "        ticker_to_screeners['TSLA'].append('Default Screener')\n",
        "\n",
        "    return ticker_to_screeners\n",
        "\n",
        "def save_results(ticker_to_screeners):\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    all_data = []\n",
        "\n",
        "    for ticker, screeners in ticker_to_screeners.items():\n",
        "        all_data.append({\n",
        "            \"Ticker\": ticker,\n",
        "            \"Appearances\": len(screeners),\n",
        "            \"Screeners\": \", \".join(screeners)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.sort_values(by=[\"Appearances\", \"Ticker\"], ascending=[False, True], inplace=True)\n",
        "\n",
        "    filename = f\"finviz_screeners_{today}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"\\nResults saved to {filename}\")\n",
        "\n",
        "def display_results(ticker_to_screeners):\n",
        "    print(\"\\n=== All Tickers Grouped by Screener ===\\n\")\n",
        "    for ticker, screeners in ticker_to_screeners.items():\n",
        "        print(f\"{ticker}: from {', '.join(screeners)}\")\n",
        "\n",
        "    print(\"\\n=== Strong Overlap Candidates (Appearing in Multiple Screeners) ===\\n\")\n",
        "    for ticker, screeners in ticker_to_screeners.items():\n",
        "        if len(screeners) > 1:\n",
        "            print(f\"{ticker}: Appears in {len(screeners)} screeners -> {', '.join(screeners)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ticker_to_screeners = aggregate_tickers(screener_urls)\n",
        "    display_results(ticker_to_screeners)\n",
        "    save_results(ticker_to_screeners)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIkOM4IMO2Ew",
        "outputId": "8da5195d-be8b-4fe5-9568-435d3cf1a6c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No new tickers found on page 2. Ending pagination.\n",
            "10% Change fetched 2 tickers.\n",
            "No new tickers found on page 2. Ending pagination.\n",
            "Growth fetched 18 tickers.\n",
            "No new tickers found on page 3. Ending pagination.\n",
            "IPO fetched 33 tickers.\n",
            "No new tickers found on page 2. Ending pagination.\n",
            "52 Week High fetched 6 tickers.\n",
            "No new tickers found on page 3. Ending pagination.\n",
            "Week 20%+ Gain fetched 32 tickers.\n",
            "\n",
            "=== All Tickers Grouped by Screener ===\n",
            "\n",
            "EPWK: from 10% Change, IPO\n",
            "UPXI: from 10% Change\n",
            "UBER: from Growth\n",
            "GRAL: from Growth, IPO, Week 20%+ Gain\n",
            "LLY: from Growth\n",
            "KGC: from Growth\n",
            "EAT: from Growth\n",
            "CI: from Growth\n",
            "ADMA: from Growth\n",
            "WGS: from Growth\n",
            "ZS: from Growth\n",
            "AGI: from Growth\n",
            "WTRG: from Growth\n",
            "CNK: from Growth\n",
            "TGTX: from Growth\n",
            "NEM: from Growth\n",
            "AEM: from Growth\n",
            "GENI: from Growth\n",
            "ATEC: from Growth\n",
            "GH: from Growth\n",
            "AHR: from IPO\n",
            "TEM: from IPO, Week 20%+ Gain\n",
            "BIRK: from IPO\n",
            "MRP: from IPO\n",
            "FATN: from IPO\n",
            "TLN: from IPO\n",
            "KRMN: from IPO\n",
            "GEV: from IPO\n",
            "USAR: from IPO\n",
            "KVUE: from IPO\n",
            "VIK: from IPO\n",
            "AS: from IPO\n",
            "AMTM: from IPO\n",
            "KVYO: from IPO\n",
            "ARM: from IPO\n",
            "RDDT: from IPO, Week 20%+ Gain\n",
            "ALAB: from IPO\n",
            "SARO: from IPO\n",
            "KGS: from IPO\n",
            "SN: from IPO\n",
            "CART: from IPO\n",
            "NNE: from IPO\n",
            "CAVA: from IPO\n",
            "RBRK: from IPO\n",
            "TBBB: from IPO\n",
            "WAY: from IPO\n",
            "CEP: from IPO, Week 20%+ Gain\n",
            "SMA: from IPO\n",
            "OS: from IPO\n",
            "LINE: from IPO\n",
            "VLTO: from IPO\n",
            "BECN: from 52 Week High\n",
            "EVRI: from 52 Week High\n",
            "TTWO: from 52 Week High\n",
            "NFLX: from 52 Week High\n",
            "DB: from 52 Week High\n",
            "EWG: from 52 Week High\n",
            "WOLF: from Week 20%+ Gain\n",
            "DNLI: from Week 20%+ Gain\n",
            "CRSR: from Week 20%+ Gain\n",
            "CRNC: from Week 20%+ Gain\n",
            "AMBP: from Week 20%+ Gain\n",
            "VERV: from Week 20%+ Gain\n",
            "WULF: from Week 20%+ Gain\n",
            "MBLY: from Week 20%+ Gain\n",
            "HSAI: from Week 20%+ Gain\n",
            "CORZ: from Week 20%+ Gain\n",
            "CSIQ: from Week 20%+ Gain\n",
            "BTDR: from Week 20%+ Gain\n",
            "SOUN: from Week 20%+ Gain\n",
            "PEGA: from Week 20%+ Gain\n",
            "LU: from Week 20%+ Gain\n",
            "PLTR: from Week 20%+ Gain\n",
            "CIFR: from Week 20%+ Gain\n",
            "MCHP: from Week 20%+ Gain\n",
            "FIP: from Week 20%+ Gain\n",
            "HOOD: from Week 20%+ Gain\n",
            "RIOT: from Week 20%+ Gain\n",
            "INDV: from Week 20%+ Gain\n",
            "TVTX: from Week 20%+ Gain\n",
            "BBAI: from Week 20%+ Gain\n",
            "DYN: from Week 20%+ Gain\n",
            "NG: from Week 20%+ Gain\n",
            "PONY: from Week 20%+ Gain\n",
            "GES: from Week 20%+ Gain\n",
            "\n",
            "=== Strong Overlap Candidates (Appearing in Multiple Screeners) ===\n",
            "\n",
            "EPWK: Appears in 2 screeners -> 10% Change, IPO\n",
            "GRAL: Appears in 3 screeners -> Growth, IPO, Week 20%+ Gain\n",
            "TEM: Appears in 2 screeners -> IPO, Week 20%+ Gain\n",
            "RDDT: Appears in 2 screeners -> IPO, Week 20%+ Gain\n",
            "CEP: Appears in 2 screeners -> IPO, Week 20%+ Gain\n",
            "\n",
            "Results saved to finviz_screeners_2025-04-25.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "print(date)\n",
        "\n",
        "# Screener URLs\n",
        "screener_urls = {\n",
        "    \"10% Change\": \"https://finviz.com/screener.ashx?v=151&f=ind_stocksonly,sh_avgvol_o500,sh_price_o5,ta_changeopen_u10,ta_sma20_sa50,ta_sma50_pa&ft=4&o=-relativevolume&c=0,1,2,3,4,5,6,64,67,65,66\",\n",
        "    \"Growth\": \"https://finviz.com/screener.ashx?v=111&f=an_recom_buybetter,fa_epsqoq_o20,fa_salesqoq_o20,ind_stocksonly,sh_avgvol_o1000,sh_price_o10,ta_perf_4wup,ta_perf2_13wup,ta_sma20_pa,ta_sma200_pa,ta_sma50_pa&ft=4\",\n",
        "    \"IPO\": \"https://finviz.com/screener.ashx?v=111&f=ind_stocksonly,ipodate_prev2yrs,sh_avgvol_o1000,sh_price_o10,ta_sma20_pa&ft=4\",\n",
        "    \"52 Week High\": \"https://finviz.com/screener.ashx?v=111&f=sh_avgvol_o1000,sh_price_o10,ta_beta_o1,ta_highlow52w_nh&ft=4\",\n",
        "    \"Week 20%+ Gain\": \"https://finviz.com/screener.ashx?v=111&f=cap_smallover,sh_avgvol_o1000,sh_price_o3,ta_perf_1w20o,ta_volatility_wo4&ft=4&o=-marketcap&r=25\"\n",
        "}\n",
        "\n",
        "def fetch_all_tickers_from_screener(screener_url, max_pages=10):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                      \"Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    session = requests.Session()\n",
        "    session.headers.update(headers)\n",
        "\n",
        "    combined_data = []\n",
        "    seen_tickers = set()\n",
        "    page = 1\n",
        "\n",
        "    while page <= max_pages:\n",
        "        paged_url = f\"{screener_url}&r={1 + (page - 1) * 20}\"\n",
        "        try:\n",
        "            response = session.get(paged_url, timeout=10)\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"Timeout fetching page {page}. Skipping...\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching page {page}: {e}\")\n",
        "            break\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        rows = soup.select('tr[valign=\"top\"]')\n",
        "\n",
        "        if not rows:\n",
        "            print(f\"No rows found on page {page}. Ending pagination.\")\n",
        "            break\n",
        "\n",
        "        page_new_data = False\n",
        "        for row in rows:\n",
        "            cells = row.find_all('td')\n",
        "            if len(cells) == 11:\n",
        "                ticker = cells[1].text.strip()\n",
        "                if ticker and ticker not in seen_tickers:\n",
        "                    combined_data.append([cell.text.strip() for cell in cells])\n",
        "                    seen_tickers.add(ticker)\n",
        "                    page_new_data = True\n",
        "\n",
        "        if not page_new_data:\n",
        "            print(f\"No new tickers found on page {page}. Ending pagination.\")\n",
        "            break\n",
        "\n",
        "        page += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "    columns = [\n",
        "        'No.', 'Ticker', 'Company', 'Sector', 'Industry',\n",
        "        'Country', 'Market Cap', 'P/E', 'Volume', 'Price', 'Change'\n",
        "    ]\n",
        "\n",
        "    if combined_data:\n",
        "        df = pd.DataFrame(combined_data, columns=columns)\n",
        "    else:\n",
        "        df = pd.DataFrame(columns=columns)\n",
        "\n",
        "    return df\n",
        "\n",
        "def aggregate_tickers(screener_urls):\n",
        "    ticker_to_screeners = defaultdict(list)\n",
        "\n",
        "    for name, url in screener_urls.items():\n",
        "        df = fetch_all_tickers_from_screener(url)\n",
        "        tickers = df['Ticker'].tolist()\n",
        "\n",
        "        print(f\"{name} fetched {len(tickers)} tickers.\")\n",
        "\n",
        "        for ticker in set(tickers):\n",
        "            ticker_to_screeners[ticker].append(name)\n",
        "\n",
        "    if not ticker_to_screeners:\n",
        "        print(\"All screeners returned no results. Adding default ticker TSLA.\")\n",
        "        ticker_to_screeners['TSLA'].append('Default Screener')\n",
        "\n",
        "    return ticker_to_screeners\n",
        "\n",
        "def save_results(ticker_to_screeners):\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    all_data = []\n",
        "\n",
        "    for ticker, screeners in ticker_to_screeners.items():\n",
        "        all_data.append({\n",
        "            \"Ticker\": ticker,\n",
        "            \"Appearances\": len(screeners),\n",
        "            \"Screeners\": \", \".join(screeners)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.sort_values(by=[\"Appearances\", \"Ticker\"], ascending=[False, True], inplace=True)\n",
        "\n",
        "    filename = f\"finviz_screeners_{today}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"\\nResults saved to {filename}\")\n",
        "\n",
        "def display_results(ticker_to_screeners):\n",
        "    print(\"\\n=== All Tickers Grouped by Screener ===\\n\")\n",
        "    for ticker, screeners in ticker_to_screeners.items():\n",
        "        print(f\"{ticker}: from {', '.join(screeners)}\")\n",
        "\n",
        "    print(\"\\n=== Strong Overlap Candidates (Appearing in Multiple Screeners) ===\\n\")\n",
        "    for ticker, screeners in ticker_to_screeners.items():\n",
        "        if len(screeners) > 1:\n",
        "            print(f\"{ticker}: Appears in {len(screeners)} screeners -> {', '.join(screeners)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ticker_to_screeners = aggregate_tickers(screener_urls)\n",
        "    display_results(ticker_to_screeners)\n",
        "    save_results(ticker_to_screeners)\n"
      ],
      "metadata": {
        "id": "lrRQKUS1BrNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1264f78-7f85-4ffd-c0d8-7080cdc60bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-04\n",
            "No new tickers found on page 2. Ending pagination.\n",
            "10% Change fetched 8 tickers.\n",
            "No new tickers found on page 3. Ending pagination.\n",
            "Growth fetched 23 tickers.\n",
            "No new tickers found on page 4. Ending pagination.\n",
            "IPO fetched 43 tickers.\n",
            "No new tickers found on page 2. Ending pagination.\n",
            "52 Week High fetched 13 tickers.\n",
            "No new tickers found on page 3. Ending pagination.\n",
            "Week 20%+ Gain fetched 23 tickers.\n",
            "\n",
            "=== All Tickers Grouped by Screener ===\n",
            "\n",
            "IONQ: from 10% Change\n",
            "DUOL: from 10% Change\n",
            "UPXI: from 10% Change\n",
            "TNXP: from 10% Change\n",
            "BULL: from 10% Change\n",
            "PRDO: from 10% Change\n",
            "TRUP: from 10% Change\n",
            "QNTM: from 10% Change\n",
            "ZS: from Growth, 52 Week High\n",
            "CI: from Growth\n",
            "CNK: from Growth\n",
            "HSAI: from Growth\n",
            "VIRT: from Growth\n",
            "ADMA: from Growth\n",
            "SE: from Growth\n",
            "UBER: from Growth\n",
            "PWR: from Growth\n",
            "BSX: from Growth\n",
            "KGC: from Growth\n",
            "BRBR: from Growth\n",
            "GENI: from Growth\n",
            "TVTX: from Growth\n",
            "KC: from Growth, Week 20%+ Gain\n",
            "GRAL: from Growth, IPO\n",
            "CPRX: from Growth\n",
            "DASH: from Growth\n",
            "WTRG: from Growth\n",
            "RBLX: from Growth\n",
            "CVNA: from Growth\n",
            "TGTX: from Growth\n",
            "APH: from Growth, 52 Week High\n",
            "EPWK: from IPO, Week 20%+ Gain\n",
            "ARM: from IPO\n",
            "SAIL: from IPO\n",
            "TACOU: from IPO\n",
            "CHACU: from IPO\n",
            "CAVA: from IPO\n",
            "NNE: from IPO\n",
            "PONY: from IPO, Week 20%+ Gain\n",
            "TBBB: from IPO\n",
            "GEV: from IPO\n",
            "AS: from IPO\n",
            "RBRK: from IPO\n",
            "KVYO: from IPO\n",
            "VIK: from IPO\n",
            "TEM: from IPO\n",
            "OS: from IPO\n",
            "CGON: from IPO, Week 20%+ Gain\n",
            "BIRK: from IPO\n",
            "KRMN: from IPO\n",
            "SN: from IPO\n",
            "VLTO: from IPO\n",
            "SARO: from IPO\n",
            "SMA: from IPO\n",
            "CHA: from IPO\n",
            "DAAQU: from IPO\n",
            "RDDT: from IPO\n",
            "AMTM: from IPO\n",
            "CRWV: from IPO, Week 20%+ Gain\n",
            "ALAB: from IPO\n",
            "WAY: from IPO\n",
            "SNDK: from IPO\n",
            "KVUE: from IPO\n",
            "AHR: from IPO, 52 Week High\n",
            "CEP: from IPO, Week 20%+ Gain\n",
            "BTSG: from IPO, Week 20%+ Gain\n",
            "SOLV: from IPO\n",
            "TVACU: from IPO\n",
            "LCCCU: from IPO\n",
            "MRP: from IPO\n",
            "CART: from IPO\n",
            "KGS: from IPO\n",
            "TLN: from IPO\n",
            "BCS: from 52 Week High\n",
            "EZU: from 52 Week High\n",
            "HWM: from 52 Week High\n",
            "SAP: from 52 Week High\n",
            "SPOT: from 52 Week High\n",
            "TGI: from 52 Week High\n",
            "APG: from 52 Week High\n",
            "EWG: from 52 Week High\n",
            "NFLX: from 52 Week High\n",
            "GFL: from 52 Week High\n",
            "RGLS: from Week 20%+ Gain\n",
            "VNET: from Week 20%+ Gain\n",
            "WOLF: from Week 20%+ Gain\n",
            "ETNB: from Week 20%+ Gain\n",
            "WVE: from Week 20%+ Gain\n",
            "ATI: from Week 20%+ Gain\n",
            "GDS: from Week 20%+ Gain\n",
            "ADPT: from Week 20%+ Gain\n",
            "COMM: from Week 20%+ Gain\n",
            "HIMS: from Week 20%+ Gain\n",
            "LEG: from Week 20%+ Gain\n",
            "JBLU: from Week 20%+ Gain\n",
            "THC: from Week 20%+ Gain\n",
            "SLDB: from Week 20%+ Gain\n",
            "ABEO: from Week 20%+ Gain\n",
            "COGT: from Week 20%+ Gain\n",
            "\n",
            "=== Strong Overlap Candidates (Appearing in Multiple Screeners) ===\n",
            "\n",
            "ZS: Appears in 2 screeners -> Growth, 52 Week High\n",
            "KC: Appears in 2 screeners -> Growth, Week 20%+ Gain\n",
            "GRAL: Appears in 2 screeners -> Growth, IPO\n",
            "APH: Appears in 2 screeners -> Growth, 52 Week High\n",
            "EPWK: Appears in 2 screeners -> IPO, Week 20%+ Gain\n",
            "PONY: Appears in 2 screeners -> IPO, Week 20%+ Gain\n",
            "CGON: Appears in 2 screeners -> IPO, Week 20%+ Gain\n",
            "CRWV: Appears in 2 screeners -> IPO, Week 20%+ Gain\n",
            "AHR: Appears in 2 screeners -> IPO, 52 Week High\n",
            "CEP: Appears in 2 screeners -> IPO, Week 20%+ Gain\n",
            "BTSG: Appears in 2 screeners -> IPO, Week 20%+ Gain\n",
            "\n",
            "Results saved to finviz_screeners_2025-05-04.csv\n",
            "CPU times: user 5.64 s, sys: 226 ms, total: 5.87 s\n",
            "Wall time: 21.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "print(date)\n",
        "\n",
        "# Screener URLs\n",
        "screener_urls = {\n",
        "    \"10% Change\": \"https://finviz.com/screener.ashx?v=151&f=ind_stocksonly,sh_avgvol_o500,sh_price_o5,ta_changeopen_u10,ta_sma20_sa50,ta_sma50_pa&ft=4&o=-relativevolume&c=0,1,2,3,4,5,6,64,67,65,66\",\n",
        "    \"Growth\": \"https://finviz.com/screener.ashx?v=111&f=an_recom_buybetter,fa_epsqoq_o20,fa_salesqoq_o20,ind_stocksonly,sh_avgvol_o1000,sh_price_o10,ta_perf_4wup,ta_perf2_13wup,ta_sma20_pa,ta_sma200_pa,ta_sma50_pa&ft=4\",\n",
        "    \"IPO\": \"https://finviz.com/screener.ashx?v=111&f=ind_stocksonly,ipodate_prev2yrs,sh_avgvol_o1000,sh_price_o10,ta_sma20_pa&ft=4\",\n",
        "    \"52 Week High\": \"https://finviz.com/screener.ashx?v=111&f=sh_avgvol_o1000,sh_price_o10,ta_beta_o1,ta_highlow52w_nh&ft=4\",\n",
        "    \"Week 20%+ Gain\": \"https://finviz.com/screener.ashx?v=111&f=cap_smallover,sh_avgvol_o1000,sh_price_o3,ta_perf_1w20o,ta_volatility_wo4&ft=4&o=-marketcap&r=25\"\n",
        "}\n",
        "\n",
        "def fetch_all_tickers_from_screener(screener_url, max_pages=10):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                      \"Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    session = requests.Session()\n",
        "    session.headers.update(headers)\n",
        "\n",
        "    combined_data = []\n",
        "    seen_tickers = set()\n",
        "    page = 1\n",
        "\n",
        "    while page <= max_pages:\n",
        "        paged_url = f\"{screener_url}&r={1 + (page - 1) * 20}\"\n",
        "        try:\n",
        "            response = session.get(paged_url, timeout=10)\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"Timeout fetching page {page}. Skipping...\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching page {page}: {e}\")\n",
        "            break\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        rows = soup.select('tr[valign=\"top\"]')\n",
        "\n",
        "        if not rows:\n",
        "            print(f\"No rows found on page {page}. Ending pagination.\")\n",
        "            break\n",
        "\n",
        "        page_new_data = False\n",
        "        for row in rows:\n",
        "            cells = row.find_all('td')\n",
        "            if len(cells) == 11:\n",
        "                ticker = cells[1].text.strip()\n",
        "                if ticker and ticker not in seen_tickers:\n",
        "                    combined_data.append([cell.text.strip() for cell in cells])\n",
        "                    seen_tickers.add(ticker)\n",
        "                    page_new_data = True\n",
        "\n",
        "        if not page_new_data:\n",
        "            print(f\"No new tickers found on page {page}. Ending pagination.\")\n",
        "            break\n",
        "\n",
        "        page += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "    columns = [\n",
        "        'No.', 'Ticker', 'Company', 'Sector', 'Industry',\n",
        "        'Country', 'Market Cap', 'P/E', 'Volume', 'Price', 'Change'\n",
        "    ]\n",
        "\n",
        "    if combined_data:\n",
        "        df = pd.DataFrame(combined_data, columns=columns)\n",
        "    else:\n",
        "        df = pd.DataFrame(columns=columns)\n",
        "\n",
        "    return df\n",
        "\n",
        "def aggregate_tickers(screener_urls):\n",
        "    ticker_to_screeners = defaultdict(list)\n",
        "\n",
        "    for name, url in screener_urls.items():\n",
        "        df = fetch_all_tickers_from_screener(url)\n",
        "        tickers = df['Ticker'].tolist()\n",
        "\n",
        "        print(f\"{name} fetched {len(tickers)} tickers.\")\n",
        "\n",
        "        for ticker in set(tickers):\n",
        "            ticker_to_screeners[ticker].append(name)\n",
        "\n",
        "    if not ticker_to_screeners:\n",
        "        print(\"All screeners returned no results. Adding default ticker TSLA.\")\n",
        "        ticker_to_screeners['TSLA'].append('Default Screener')\n",
        "\n",
        "    return ticker_to_screeners\n",
        "\n",
        "def save_results(ticker_to_screeners):\n",
        "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    all_data = []\n",
        "\n",
        "    for ticker, screeners in ticker_to_screeners.items():\n",
        "        all_data.append({\n",
        "            \"Ticker\": ticker,\n",
        "            \"Appearances\": len(screeners),\n",
        "            \"Screeners\": \", \".join(screeners)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.sort_values(by=[\"Appearances\", \"Ticker\"], ascending=[False, True], inplace=True)\n",
        "\n",
        "    filename = f\"finviz_screeners_{today}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"\\nResults saved to {filename}\")\n",
        "\n",
        "def display_results(ticker_to_screeners):\n",
        "    print(\"\\n=== All Tickers Grouped by Screener ===\\n\")\n",
        "    for ticker, screeners in ticker_to_screeners.items():\n",
        "        print(f\"{ticker}: from {', '.join(screeners)}\")\n",
        "\n",
        "    print(\"\\n=== Strong Overlap Candidates (Appearing in Multiple Screeners) ===\\n\")\n",
        "    for ticker, screeners in ticker_to_screeners.items():\n",
        "        if len(screeners) > 1:\n",
        "            print(f\"{ticker}: Appears in {len(screeners)} screeners -> {', '.join(screeners)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ticker_to_screeners = aggregate_tickers(screener_urls)\n",
        "    display_results(ticker_to_screeners)\n",
        "    save_results(ticker_to_screeners)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEMjyfKzMnGi",
        "outputId": "e27487de-0303-43e2-c0d2-46b10f2ca8a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-07\n",
            "No new tickers found on page 2. Ending pagination.\n",
            "10% Change fetched 9 tickers.\n",
            "No new tickers found on page 3. Ending pagination.\n",
            "Growth fetched 26 tickers.\n",
            "No new tickers found on page 4. Ending pagination.\n",
            "IPO fetched 42 tickers.\n",
            "No new tickers found on page 2. Ending pagination.\n",
            "52 Week High fetched 4 tickers.\n",
            "No new tickers found on page 3. Ending pagination.\n",
            "Week 20%+ Gain fetched 21 tickers.\n",
            "\n",
            "=== All Tickers Grouped by Screener ===\n",
            "\n",
            "AEVA: from 10% Change, Week 20%+ Gain\n",
            "DFDV: from 10% Change\n",
            "RGC: from 10% Change\n",
            "TDUP: from 10% Change, Week 20%+ Gain\n",
            "ULS: from 10% Change\n",
            "IHS: from 10% Change\n",
            "EPWK: from 10% Change\n",
            "EPSM: from 10% Change\n",
            "CGAU: from 10% Change\n",
            "CTRE: from Growth\n",
            "SE: from Growth\n",
            "FUTU: from Growth\n",
            "BSX: from Growth\n",
            "DTE: from Growth\n",
            "KC: from Growth, Week 20%+ Gain\n",
            "WTRG: from Growth\n",
            "CVNA: from Growth\n",
            "UBER: from Growth\n",
            "DUOL: from Growth, Week 20%+ Gain\n",
            "EQT: from Growth\n",
            "KGC: from Growth\n",
            "EGO: from Growth\n",
            "BCRX: from Growth, Week 20%+ Gain\n",
            "AEM: from Growth\n",
            "VIRT: from Growth\n",
            "WELL: from Growth\n",
            "GENI: from Growth\n",
            "PWR: from Growth\n",
            "GFI: from Growth\n",
            "ADMA: from Growth\n",
            "ZS: from Growth\n",
            "APH: from Growth\n",
            "NEM: from Growth\n",
            "HSAI: from Growth\n",
            "GRAL: from Growth, IPO\n",
            "CEP: from IPO\n",
            "CART: from IPO\n",
            "SARO: from IPO\n",
            "TVACU: from IPO\n",
            "NPACU: from IPO\n",
            "CGCTU: from IPO\n",
            "MRP: from IPO\n",
            "SMA: from IPO\n",
            "VIK: from IPO\n",
            "KVYO: from IPO\n",
            "RDAGU: from IPO\n",
            "LCCCU: from IPO\n",
            "AHR: from IPO, 52 Week High\n",
            "BIRK: from IPO\n",
            "CHACU: from IPO\n",
            "KGS: from IPO\n",
            "PONY: from IPO, Week 20%+ Gain\n",
            "CAVA: from IPO\n",
            "RBRK: from IPO\n",
            "TLN: from IPO\n",
            "SAIL: from IPO\n",
            "TEM: from IPO\n",
            "ALAB: from IPO\n",
            "SNDK: from IPO\n",
            "CRWV: from IPO, Week 20%+ Gain\n",
            "GEV: from IPO\n",
            "KRMN: from IPO\n",
            "WAY: from IPO\n",
            "RDDT: from IPO\n",
            "ARM: from IPO\n",
            "COPL-U: from IPO\n",
            "VLTO: from IPO\n",
            "AS: from IPO\n",
            "GTENU: from IPO\n",
            "OS: from IPO\n",
            "BTSG: from IPO, Week 20%+ Gain\n",
            "NNE: from IPO\n",
            "AMTM: from IPO\n",
            "TACOU: from IPO\n",
            "CHA: from IPO\n",
            "SN: from IPO\n",
            "APG: from 52 Week High\n",
            "HWM: from 52 Week High\n",
            "AER: from 52 Week High\n",
            "ATI: from Week 20%+ Gain\n",
            "GTX: from Week 20%+ Gain\n",
            "HIMS: from Week 20%+ Gain\n",
            "CRMD: from Week 20%+ Gain\n",
            "WRD: from Week 20%+ Gain\n",
            "XRX: from Week 20%+ Gain\n",
            "VNET: from Week 20%+ Gain\n",
            "EXAS: from Week 20%+ Gain\n",
            "NGD: from Week 20%+ Gain\n",
            "CEG: from Week 20%+ Gain\n",
            "COMM: from Week 20%+ Gain\n",
            "TNDM: from Week 20%+ Gain\n",
            "EVGO: from Week 20%+ Gain\n",
            "\n",
            "=== Strong Overlap Candidates (Appearing in Multiple Screeners) ===\n",
            "\n",
            "AEVA: Appears in 2 screeners -> 10% Change, Week 20%+ Gain\n",
            "TDUP: Appears in 2 screeners -> 10% Change, Week 20%+ Gain\n",
            "KC: Appears in 2 screeners -> Growth, Week 20%+ Gain\n",
            "DUOL: Appears in 2 screeners -> Growth, Week 20%+ Gain\n",
            "BCRX: Appears in 2 screeners -> Growth, Week 20%+ Gain\n",
            "GRAL: Appears in 2 screeners -> Growth, IPO\n",
            "AHR: Appears in 2 screeners -> IPO, 52 Week High\n",
            "PONY: Appears in 2 screeners -> IPO, Week 20%+ Gain\n",
            "CRWV: Appears in 2 screeners -> IPO, Week 20%+ Gain\n",
            "BTSG: Appears in 2 screeners -> IPO, Week 20%+ Gain\n",
            "\n",
            "Results saved to finviz_screeners_2025-05-07.csv\n",
            "CPU times: user 5.58 s, sys: 246 ms, total: 5.83 s\n",
            "Wall time: 20.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KM34ucvmj3xc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}